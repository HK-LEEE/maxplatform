# LLM ê¶Œí•œ ì‹œìŠ¤í…œ ê°œì„  ì„¤ê³„

## ğŸ¯ ê°œìš”

AdminPageì—ì„œ ë°œìƒí•˜ëŠ” LLM ëª¨ë¸ ê¶Œí•œ ê´€ë¦¬ ì˜¤ë¥˜ë“¤ì„ ì²´ê³„ì ìœ¼ë¡œ í•´ê²°í•˜ê¸° ìœ„í•œ ì„¤ê³„ ë¬¸ì„œì…ë‹ˆë‹¤.

### ì‹ë³„ëœ ì£¼ìš” ë¬¸ì œì 
1. **ì¸ì¦ í† í° ì „ë‹¬ ì˜¤ë¥˜** (401 Unauthorized)
2. **ë°ì´í„°ë² ì´ìŠ¤/ë°±ì—”ë“œ ì˜¤ë¥˜** (500 Internal Server Error)  
3. **í¬íŠ¸ ì„¤ì • ë¶ˆì¼ì¹˜** (3000 vs 8000)
4. **ì„±ëŠ¥ ë° ì•ˆì •ì„± ë¬¸ì œ**

---

## ğŸ”’ Wave 1: ì¸ì¦ ë° í† í° ê´€ë¦¬ ê°œì„ 

### 1.1 í˜„ì¬ ë¬¸ì œì  ë¶„ì„

```typescript
// âŒ í˜„ì¬ AdminPage.tsxì˜ ë¬¸ì œì ë“¤
const permissions = await fetch(`/api/llm-models/${model.id}/permissions`, {
  headers: { Authorization: `Bearer ${localStorage.getItem('token')}` }
});

const existingPermissions = await fetch(`/api/llm-models?accessible_only=false`, {
  headers: { Authorization: `Bearer ${token}` }
});
```

**ë¬¸ì œì :**
- ì¼ê´€ì„± ì—†ëŠ” API í˜¸ì¶œ íŒ¨í„´
- í† í° ë§Œë£Œ ì²˜ë¦¬ ë¶€ì¬
- ì—ëŸ¬ í•¸ë“¤ë§ ë¯¸í¡
- ì§ì ‘ì ì¸ localStorage ì ‘ê·¼

### 1.2 ì„¤ê³„ ì†”ë£¨ì…˜: Unified API Client

```typescript
// ğŸ“ frontend/src/services/apiClient.ts
class APIClient {
  private baseURL: string;
  private tokenManager: TokenManager;

  constructor() {
    this.baseURL = '';  // Vite proxy í™œìš©
    this.tokenManager = new TokenManager();
  }

  async request<T>(
    endpoint: string, 
    options: RequestOptions = {}
  ): Promise<APIResponse<T>> {
    const { method = 'GET', data, headers = {} } = options;
    
    // 1. í† í° ìë™ ì²¨ë¶€
    const token = await this.tokenManager.getValidToken();
    if (token) {
      headers['Authorization'] = `Bearer ${token}`;
    }

    // 2. ì¼ê´€ëœ í—¤ë” ì„¤ì •
    headers['Content-Type'] = 'application/json';

    try {
      const response = await fetch(`${this.baseURL}${endpoint}`, {
        method,
        headers,
        body: data ? JSON.stringify(data) : undefined,
      });

      // 3. ì¸ì¦ ì˜¤ë¥˜ ìë™ ì²˜ë¦¬
      if (response.status === 401) {
        await this.tokenManager.refreshToken();
        // ì¬ì‹œë„ ë¡œì§
        return this.request(endpoint, options);
      }

      // 4. í‘œì¤€í™”ëœ ì—ëŸ¬ ì²˜ë¦¬
      if (!response.ok) {
        throw new APIError(response.status, await response.text());
      }

      return {
        data: await response.json(),
        status: response.status,
        success: true
      };

    } catch (error) {
      return this.handleError(error, endpoint);
    }
  }

  // LLM ëª¨ë¸ ê¶Œí•œ ì „ìš© ë©”ì„œë“œë“¤
  async getLLMModelPermissions(modelId: string) {
    return this.request<ModelPermission[]>(
      `/api/llm-models/${modelId}/permissions`
    );
  }

  async getAllLLMModels(accessibleOnly: boolean = false) {
    return this.request<LLMModel[]>(
      `/api/llm-models?accessible_only=${accessibleOnly}`
    );
  }

  async grantModelPermission(modelId: string, permission: PermissionCreate) {
    return this.request<ModelPermission>(
      `/api/llm-models/${modelId}/permissions`,
      { method: 'POST', data: permission }
    );
  }
}
```

### 1.3 í† í° ë§¤ë‹ˆì € ì„¤ê³„

```typescript
// ğŸ“ frontend/src/services/tokenManager.ts
class TokenManager {
  private token: string | null = null;
  private refreshToken: string | null = null;
  private tokenExpiry: number | null = null;

  constructor() {
    this.loadTokens();
  }

  async getValidToken(): Promise<string | null> {
    // 1. í† í° ì¡´ì¬ ë° ë§Œë£Œ í™•ì¸
    if (!this.token || this.isTokenExpiring()) {
      await this.refreshToken();
    }
    return this.token;
  }

  private isTokenExpiring(): boolean {
    if (!this.tokenExpiry) return true;
    // 5ë¶„ ì „ì— ë¯¸ë¦¬ ê°±ì‹ 
    return Date.now() >= (this.tokenExpiry - 5 * 60 * 1000);
  }

  async refreshToken(): Promise<void> {
    try {
      const response = await fetch('/api/auth/refresh', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ refresh_token: this.refreshToken })
      });

      if (response.ok) {
        const { access_token, expires_in } = await response.json();
        this.setToken(access_token, expires_in);
      } else {
        this.clearTokens();
        // ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
        window.location.href = '/login';
      }
    } catch (error) {
      console.error('Token refresh failed:', error);
      this.clearTokens();
    }
  }

  private setToken(token: string, expiresIn: number): void {
    this.token = token;
    this.tokenExpiry = Date.now() + (expiresIn * 1000);
    localStorage.setItem('access_token', token);
    localStorage.setItem('token_expiry', this.tokenExpiry.toString());
  }

  private loadTokens(): void {
    this.token = localStorage.getItem('access_token');
    this.refreshToken = localStorage.getItem('refresh_token');
    const expiry = localStorage.getItem('token_expiry');
    this.tokenExpiry = expiry ? parseInt(expiry) : null;
  }

  private clearTokens(): void {
    this.token = null;
    this.refreshToken = null;
    this.tokenExpiry = null;
    localStorage.removeItem('access_token');
    localStorage.removeItem('refresh_token');
    localStorage.removeItem('token_expiry');
  }
}
```

### 1.4 AdminPage ë¦¬íŒ©í† ë§

```typescript
// ğŸ“ frontend/src/pages/AdminPage.tsx (ê°œì„ ëœ ë²„ì „)
import { apiClient } from '../services/apiClient';

// âœ… ê°œì„ ëœ ê·¸ë£¹ LLM ëª¨ë¸ ê¶Œí•œ ë¡œë”©
const loadGroupModelPermissions = async (groupId: string) => {
  try {
    console.log('ê·¸ë£¹ LLM ëª¨ë¸ ê¶Œí•œ ì¡°íšŒ ì‹œì‘:', groupId);
    const assignedModelIds: string[] = [];
    const assignedModelsList: LLMModelManagement[] = [];
    
    for (const model of llmModels) {
      try {
        const response = await apiClient.getLLMModelPermissions(model.id);
        
        if (response.success) {
          const hasGroupPermission = response.data.some(p => 
            p.grantee_type === 'GROUP' && p.grantee_id === groupId
          );
          
          if (hasGroupPermission) {
            assignedModelIds.push(model.id);
            assignedModelsList.push(model);
          }
        }
      } catch (error) {
        console.error(`ëª¨ë¸ ${model.id} ê¶Œí•œ ì¡°íšŒ ì‹¤íŒ¨:`, error);
        // ê°œë³„ ëª¨ë¸ ì‹¤íŒ¨ëŠ” ì „ì²´ ë¡œë”©ì„ ì¤‘ë‹¨í•˜ì§€ ì•ŠìŒ
      }
    }

    setSelectedGroupModels(assignedModelIds);
    setSelectedGroupModelsList(assignedModelsList);
    
  } catch (error) {
    console.error('ê·¸ë£¹ ëª¨ë¸ ê¶Œí•œ ë¡œë”© ì‹¤íŒ¨:', error);
    showNotification('ê·¸ë£¹ ëª¨ë¸ ê¶Œí•œì„ ë¶ˆëŸ¬ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.', 'error');
  }
};

// âœ… ê°œì„ ëœ ê·¸ë£¹ ì €ì¥ ë¡œì§
const saveGroup = async () => {
  try {
    // ... ê¸°ì¡´ ê·¸ë£¹ ì €ì¥ ë¡œì§ ...

    // LLM ëª¨ë¸ ê¶Œí•œ ì—…ë°ì´íŠ¸
    if (selectedGroup) {
      // ê¸°ì¡´ ê¶Œí•œ ì¡°íšŒ ë° ì‚­ì œ
      const modelsResponse = await apiClient.getAllLLMModels(false);
      
      if (modelsResponse.success) {
        // ê° ëª¨ë¸ì˜ ê·¸ë£¹ ê¶Œí•œ ì‚­ì œ
        for (const model of modelsResponse.data) {
          try {
            const permissionsResponse = await apiClient.getLLMModelPermissions(model.id);
            
            if (permissionsResponse.success) {
              const groupPermission = permissionsResponse.data.find(p => 
                p.grantee_type === 'GROUP' && p.grantee_id === groupData.id
              );
              
              if (groupPermission) {
                await apiClient.request(
                  `/api/llm-models/${model.id}/permissions/${groupPermission.id}`,
                  { method: 'DELETE' }
                );
              }
            }
          } catch (error) {
            console.error(`ëª¨ë¸ ${model.id} ê¶Œí•œ ì‚­ì œ ì‹¤íŒ¨:`, error);
          }
        }
      }
    }

    // ìƒˆë¡œ ì„ íƒëœ ëª¨ë¸ë“¤ì— ê¶Œí•œ ë¶€ì—¬
    for (const modelId of selectedGroupModels) {
      try {
        await apiClient.grantModelPermission(modelId, {
          model_id: modelId,
          grantee_type: 'GROUP',
          grantee_id: groupData.id
        });
      } catch (error) {
        console.error(`ëª¨ë¸ ${modelId} ê¶Œí•œ ë¶€ì—¬ ì‹¤íŒ¨:`, error);
      }
    }

    await fetchData();
    setIsGroupModalOpen(false);
    showNotification('ê·¸ë£¹ì´ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.', 'success');

  } catch (error) {
    console.error('ê·¸ë£¹ ì €ì¥ ì‹¤íŒ¨:', error);
    showNotification('ê·¸ë£¹ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.', 'error');
  }
};
```

### 1.5 ì—ëŸ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ

```typescript
// ğŸ“ frontend/src/types/api.ts
interface APIResponse<T> {
  data: T;
  status: number;
  success: boolean;
  error?: string;
}

class APIError extends Error {
  constructor(
    public status: number,
    public message: string,
    public endpoint?: string
  ) {
    super(message);
    this.name = 'APIError';
  }
}

// ğŸ“ frontend/src/services/errorHandler.ts
export class ErrorHandler {
  static handle(error: APIError, context: string): void {
    console.error(`API Error in ${context}:`, error);

    switch (error.status) {
      case 401:
        // ì¸ì¦ ì˜¤ë¥˜ - ìë™ ë¡œê·¸ì•„ì›ƒ
        this.handleAuthError();
        break;
      case 403:
        showNotification('ì ‘ê·¼ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤.', 'error');
        break;
      case 404:
        showNotification('ìš”ì²­í•œ ë¦¬ì†ŒìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.', 'error');
        break;
      case 500:
        showNotification('ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.', 'error');
        break;
      default:
        showNotification('ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.', 'error');
    }
  }

  private static handleAuthError(): void {
    localStorage.clear();
    showNotification('ë¡œê·¸ì¸ì´ ë§Œë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ë¡œê·¸ì¸í•´ì£¼ì„¸ìš”.', 'warning');
    window.location.href = '/login';
  }
}
```

---

## ğŸ“Š Wave 1 êµ¬í˜„ ìš°ì„ ìˆœìœ„

| ì»´í¬ë„ŒíŠ¸ | ìš°ì„ ìˆœìœ„ | ì˜ˆìƒ ì‹œê°„ | ì˜í–¥ë„ |
|---------|---------|---------|--------|
| TokenManager | High | 4h | Critical |
| APIClient | High | 6h | Critical |
| AdminPage ë¦¬íŒ©í† ë§ | High | 8h | High |
| ErrorHandler | Medium | 2h | Medium |

### Wave 1 ì™„ë£Œ ê¸°ì¤€
- âœ… ëª¨ë“  API í˜¸ì¶œì´ APIClientë¥¼ í†µí•´ ì´ë£¨ì–´ì§
- âœ… í† í° ë§Œë£Œ ì‹œ ìë™ ê°±ì‹  ê¸°ëŠ¥ ì‘ë™
- âœ… 401 ì˜¤ë¥˜ ë°œìƒ ì‹œ ìë™ ì¬ì‹œë„ ë˜ëŠ” ë¡œê·¸ì•„ì›ƒ
- âœ… ì¼ê´€ëœ ì—ëŸ¬ ë©”ì‹œì§€ í‘œì‹œ

---

## ğŸ”§ Wave 2: ë°±ì—”ë“œ ì—ëŸ¬ ì²˜ë¦¬ ë° ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”

### 2.1 í˜„ì¬ ë¬¸ì œì  ë¶„ì„

**ë°ì´í„°ë² ì´ìŠ¤ ì—°ê´€ 500 ì—ëŸ¬:**
```
2025-07-30 08:38:10.332 [INFO] uvicorn.access:496 - 127.0.0.1:41494 - "GET /api/llm-models/4a807dd5-d62a-45e2-a6b2-45b40c39903f/permissions HTTP/1.1" 500
```

**ì‹ë³„ëœ ë¬¸ì œì :**
- íŠ¹ì • ëª¨ë¸ IDì— ëŒ€í•œ ì¼ê´€ëœ ì‹¤íŒ¨
- íƒ€ì„ì•„ì›ƒ ë°œìƒ (curl ìš”ì²­ì´ 2ë¶„ê°„ ëŒ€ê¸°)
- ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í’€ ê³ ê°ˆ ê°€ëŠ¥ì„±
- ì—ëŸ¬ ë¡œê¹… ë¶€ì¡±ìœ¼ë¡œ ë””ë²„ê¹… ì–´ë ¤ì›€

### 2.2 ì„¤ê³„ ì†”ë£¨ì…˜: ê°•í™”ëœ ë°±ì—”ë“œ ì•„í‚¤í…ì²˜

```python
# ğŸ“ backend/app/services/llm_permissions_service.py
from contextlib import asynccontextmanager
from sqlalchemy.exc import SQLAlchemyError, DatabaseError
from sqlalchemy import text
import asyncio
from typing import Optional, List, Dict, Any
import logging

logger = logging.getLogger(__name__)

class LLMPermissionsService:
    """LLM ëª¨ë¸ ê¶Œí•œ ê´€ë¦¬ ì „ìš© ì„œë¹„ìŠ¤"""
    
    def __init__(self, db_session_factory):
        self.db_session_factory = db_session_factory
        self.circuit_breaker = CircuitBreaker(
            failure_threshold=5,
            recovery_timeout=30,
            expected_exception=DatabaseError
        )

    @asynccontextmanager
    async def get_db_session(self):
        """ì•ˆì „í•œ ë°ì´í„°ë² ì´ìŠ¤ ì„¸ì…˜ ê´€ë¦¬"""
        session = None
        try:
            session = self.db_session_factory()
            yield session
            session.commit()
        except SQLAlchemyError as e:
            if session:
                session.rollback()
            logger.error(f"Database error: {str(e)}", exc_info=True)
            raise
        except Exception as e:
            if session:
                session.rollback()
            logger.error(f"Unexpected error: {str(e)}", exc_info=True)
            raise
        finally:
            if session:
                session.close()

    async def get_model_permissions_with_circuit_breaker(
        self, 
        model_id: str,
        timeout: int = 10
    ) -> List[Dict[str, Any]]:
        """Circuit breaker íŒ¨í„´ì„ ì ìš©í•œ ê¶Œí•œ ì¡°íšŒ"""
        
        @self.circuit_breaker
        async def _get_permissions():
            return await asyncio.wait_for(
                self._get_model_permissions_internal(model_id),
                timeout=timeout
            )
        
        try:
            return await _get_permissions()
        except asyncio.TimeoutError:
            logger.warning(f"Timeout getting permissions for model {model_id}")
            raise HTTPException(
                status_code=status.HTTP_408_REQUEST_TIMEOUT,
                detail=f"Permission query timeout for model {model_id}"
            )
        except CircuitBreakerError:
            logger.error(f"Circuit breaker open for model {model_id}")
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="Permission service temporarily unavailable"
            )

    async def _get_model_permissions_internal(self, model_id: str) -> List[Dict[str, Any]]:
        """ë‚´ë¶€ ê¶Œí•œ ì¡°íšŒ ë¡œì§ (ìµœì í™”ë¨)"""
        async with self.get_db_session() as db:
            try:
                # 1. ëª¨ë¸ ì¡´ì¬ í™•ì¸ (ë¹ ë¥¸ ì²´í¬)
                model_exists = await self._check_model_exists(db, model_id)
                if not model_exists:
                    raise HTTPException(
                        status_code=status.HTTP_404_NOT_FOUND,
                        detail=f"Model {model_id} not found"
                    )

                # 2. ìµœì í™”ëœ ê¶Œí•œ ì¡°íšŒ ì¿¼ë¦¬
                permissions_query = text("""
                    SELECT 
                        p.id,
                        p.model_id,
                        p.grantee_type,
                        p.grantee_id,
                        COALESCE(u.full_name, g.name, p.grantee_id) as grantee_name,
                        p.granted_by,
                        granter.full_name as granted_by_name,
                        p.created_at,
                        p.updated_at
                    FROM maxllm_model_permissions p
                    LEFT JOIN users u ON p.grantee_type = 'USER' AND p.grantee_id = u.id::text
                    LEFT JOIN groups g ON p.grantee_type = 'GROUP' AND p.grantee_id = g.id::text
                    LEFT JOIN users granter ON p.granted_by = granter.id
                    WHERE p.model_id = :model_id
                    ORDER BY p.created_at DESC
                """)
                
                result = db.execute(permissions_query, {"model_id": model_id})
                permissions = []
                
                for row in result:
                    permissions.append({
                        "id": row.id,
                        "model_id": row.model_id,
                        "grantee_type": row.grantee_type,
                        "grantee_id": row.grantee_id,
                        "grantee_name": row.grantee_name,
                        "granted_by": str(row.granted_by),
                        "granted_by_name": row.granted_by_name,
                        "created_at": row.created_at.isoformat() if row.created_at else None,
                        "updated_at": row.updated_at.isoformat() if row.updated_at else None
                    })
                
                logger.info(f"Successfully retrieved {len(permissions)} permissions for model {model_id}")
                return permissions
                
            except SQLAlchemyError as e:
                logger.error(f"Database error retrieving permissions for model {model_id}: {str(e)}")
                raise
            except Exception as e:
                logger.error(f"Unexpected error retrieving permissions for model {model_id}: {str(e)}")
                raise

    async def _check_model_exists(self, db, model_id: str) -> bool:
        """ëª¨ë¸ ì¡´ì¬ ì—¬ë¶€ ë¹ ë¥¸ í™•ì¸"""
        try:
            existence_query = text("""
                SELECT 1 FROM maxllm_models 
                WHERE id = :model_id AND is_active = true
                LIMIT 1
            """)
            result = db.execute(existence_query, {"model_id": model_id})
            return result.first() is not None
        except SQLAlchemyError as e:
            logger.error(f"Error checking model existence {model_id}: {str(e)}")
            return False

    async def grant_permission_with_validation(
        self,
        model_id: str,
        grantee_type: str,
        grantee_id: str,
        granted_by: str
    ) -> Dict[str, Any]:
        """ê²€ì¦ê³¼ í•¨ê»˜ ê¶Œí•œ ë¶€ì—¬"""
        async with self.get_db_session() as db:
            try:
                # 1. ëª¨ë¸ ì¡´ì¬ ë° ê¶Œí•œ í™•ì¸
                model = await self._get_model_with_permissions(db, model_id, granted_by)
                
                # 2. ê¶Œí•œ ëŒ€ìƒì ì¡´ì¬ í™•ì¸
                grantee = await self._validate_grantee(db, grantee_type, grantee_id)
                
                # 3. ì¤‘ë³µ ê¶Œí•œ í™•ì¸
                existing = await self._check_existing_permission(
                    db, model_id, grantee_type, grantee_id
                )
                if existing:
                    raise HTTPException(
                        status_code=status.HTTP_409_CONFLICT,
                        detail="Permission already exists"
                    )
                
                # 4. ê¶Œí•œ ìƒì„±
                new_permission = await self._create_permission(
                    db, model_id, grantee_type, grantee_id, granted_by
                )
                
                logger.info(f"Permission granted: {model_id} -> {grantee_type}:{grantee_id}")
                return new_permission
                
            except HTTPException:
                raise
            except Exception as e:
                logger.error(f"Error granting permission: {str(e)}")
                raise HTTPException(
                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                    detail="Failed to grant permission"
                )
```

### 2.3 Circuit Breaker íŒ¨í„´ êµ¬í˜„

```python
# ğŸ“ backend/app/utils/circuit_breaker.py
import time
import asyncio
from enum import Enum
from typing import Callable, Any, Type, Optional
import logging

logger = logging.getLogger(__name__)

class CircuitState(Enum):
    CLOSED = "closed"
    OPEN = "open"
    HALF_OPEN = "half_open"

class CircuitBreakerError(Exception):
    pass

class CircuitBreaker:
    """Database ì—°ê²° ì‹¤íŒ¨ë¥¼ ìœ„í•œ Circuit Breaker"""
    
    def __init__(
        self,
        failure_threshold: int = 5,
        recovery_timeout: int = 30,
        expected_exception: Type[Exception] = Exception
    ):
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.expected_exception = expected_exception
        
        self.failure_count = 0
        self.last_failure_time = None
        self.state = CircuitState.CLOSED

    def __call__(self, func: Callable) -> Callable:
        """ë°ì½”ë ˆì´í„°ë¡œ ì‚¬ìš©"""
        async def wrapper(*args, **kwargs):
            if self.state == CircuitState.OPEN:
                if self._should_attempt_reset():
                    self.state = CircuitState.HALF_OPEN
                    logger.info("Circuit breaker half-open, attempting recovery")
                else:
                    raise CircuitBreakerError("Circuit breaker is open")

            try:
                result = await func(*args, **kwargs)
                self._on_success()
                return result
            except self.expected_exception as e:
                self._on_failure()
                raise e

        return wrapper

    def _should_attempt_reset(self) -> bool:
        """ë³µêµ¬ ì‹œë„ ì—¬ë¶€ íŒë‹¨"""
        return (
            self.last_failure_time and
            time.time() - self.last_failure_time >= self.recovery_timeout
        )

    def _on_success(self):
        """ì„±ê³µ ì‹œ ì²˜ë¦¬"""
        self.failure_count = 0
        self.state = CircuitState.CLOSED
        logger.debug("Circuit breaker reset to closed")

    def _on_failure(self):
        """ì‹¤íŒ¨ ì‹œ ì²˜ë¦¬"""
        self.failure_count += 1
        self.last_failure_time = time.time()
        
        if self.failure_count >= self.failure_threshold:
            self.state = CircuitState.OPEN
            logger.warning(f"Circuit breaker opened after {self.failure_count} failures")
```

### 2.4 ê°œì„ ëœ API ì—”ë“œí¬ì¸íŠ¸

```python
# ğŸ“ backend/app/api/llm_models.py (ê°œì„ ëœ ë²„ì „)
from ..services.llm_permissions_service import LLMPermissionsService
from ..utils.request_context import get_request_context
import uuid

# ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
permissions_service = LLMPermissionsService(get_db)

@router.get("/{model_id}/permissions", response_model=List[ModelPermissionResponse])
async def get_model_permissions_v2(
    model_id: str,
    current_user: User = Depends(get_current_user),
    request_id: str = Depends(get_request_context)
):
    """ê°œì„ ëœ ëª¨ë¸ ê¶Œí•œ ì¡°íšŒ API"""
    logger.info(f"[{request_id}] Getting permissions for model {model_id} by user {current_user.id}")
    
    try:
        # UUID í˜•ì‹ ê²€ì¦
        try:
            uuid.UUID(model_id)
        except ValueError:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Invalid model ID format"
            )
        
        # ê¶Œí•œ ì¡°íšŒ (Circuit Breaker ì ìš©)
        permissions = await permissions_service.get_model_permissions_with_circuit_breaker(
            model_id=model_id
        )
        
        # ê´€ë¦¬ ê¶Œí•œ í™•ì¸ (ë¹„ë™ê¸°)
        has_manage_permission = await permissions_service.check_manage_permission(
            model_id=model_id,
            user_id=str(current_user.id)
        )
        
        if not has_manage_permission:
            logger.warning(f"[{request_id}] User {current_user.id} lacks manage permission for model {model_id}")
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="Insufficient permissions to view model permissions"
            )
        
        logger.info(f"[{request_id}] Successfully retrieved {len(permissions)} permissions")
        return permissions
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[{request_id}] Unexpected error: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Internal server error retrieving permissions"
        )

@router.post("/{model_id}/permissions", response_model=ModelPermissionResponse)
async def grant_model_permission_v2(
    model_id: str,
    permission_data: ModelPermissionCreate,
    current_user: User = Depends(get_current_user),
    request_id: str = Depends(get_request_context)
):
    """ê°œì„ ëœ ëª¨ë¸ ê¶Œí•œ ë¶€ì—¬ API"""
    logger.info(f"[{request_id}] Granting permission for model {model_id} by user {current_user.id}")
    
    try:
        # ì…ë ¥ ê²€ì¦
        if not permission_data.grantee_id or not permission_data.grantee_type:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Grantee ID and type are required"
            )
        
        # ê¶Œí•œ ë¶€ì—¬ (íŠ¸ëœì­ì…˜ ì ìš©)
        permission = await permissions_service.grant_permission_with_validation(
            model_id=model_id,
            grantee_type=permission_data.grantee_type.value,
            grantee_id=permission_data.grantee_id,
            granted_by=str(current_user.id)
        )
        
        logger.info(f"[{request_id}] Permission granted successfully")
        return permission
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"[{request_id}] Error granting permission: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Failed to grant permission"
        )
```

### 2.5 ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”

```sql
-- ğŸ“ backend/migrations/005_optimize_llm_permissions.sql
-- LLM ëª¨ë¸ ê¶Œí•œ ì‹œìŠ¤í…œ ìµœì í™”

-- 1. ë³µí•© ì¸ë±ìŠ¤ ì¶”ê°€ (ì„±ëŠ¥ í–¥ìƒ)
CREATE INDEX IF NOT EXISTS idx_model_permissions_lookup 
ON maxllm_model_permissions (model_id, grantee_type, grantee_id);

CREATE INDEX IF NOT EXISTS idx_model_permissions_grantee 
ON maxllm_model_permissions (grantee_type, grantee_id);

CREATE INDEX IF NOT EXISTS idx_models_active_owner 
ON maxllm_models (is_active, owner_type, owner_id);

-- 2. ë¶€ë¶„ ì¸ë±ìŠ¤ (í™œì„± ëª¨ë¸ë§Œ)
CREATE INDEX IF NOT EXISTS idx_models_active_only 
ON maxllm_models (id) WHERE is_active = true;

-- 3. ê¶Œí•œ ì¡°íšŒ ìµœì í™”ë¥¼ ìœ„í•œ ë·°
CREATE OR REPLACE VIEW v_model_permissions_detailed AS
SELECT 
    p.id,
    p.model_id,
    m.model_name,
    p.grantee_type,
    p.grantee_id,
    CASE 
        WHEN p.grantee_type = 'USER' THEN u.full_name
        WHEN p.grantee_type = 'GROUP' THEN g.name
        ELSE p.grantee_id
    END as grantee_name,
    p.granted_by,
    granter.full_name as granted_by_name,
    p.created_at,
    p.updated_at
FROM maxllm_model_permissions p
JOIN maxllm_models m ON p.model_id = m.id AND m.is_active = true
LEFT JOIN users u ON p.grantee_type = 'USER' AND p.grantee_id = u.id::text
LEFT JOIN groups g ON p.grantee_type = 'GROUP' AND p.grantee_id = g.id::text
LEFT JOIN users granter ON p.granted_by = granter.id;

-- 4. ì—°ê²° í’€ ìµœì í™” ì„¤ì •
-- postgresql.conf ê¶Œì¥ ì„¤ì •:
-- max_connections = 200
-- shared_buffers = 256MB
-- effective_cache_size = 1GB
-- work_mem = 4MB
-- maintenance_work_mem = 64MB
```

### 2.6 Request Context ë° ë¡œê¹… ê°œì„ 

```python
# ğŸ“ backend/app/utils/request_context.py
import uuid
import contextvars
from fastapi import Request, Depends
import time

# Request ID context variable
request_id_ctx: contextvars.ContextVar[str] = contextvars.ContextVar('request_id')

def get_request_context():
    """Request ID ìƒì„± ë° ì„¤ì •"""
    request_id = str(uuid.uuid4())[:8]
    request_id_ctx.set(request_id)
    return request_id

def get_current_request_id() -> str:
    """í˜„ì¬ Request ID ì¡°íšŒ"""
    try:
        return request_id_ctx.get()
    except LookupError:
        return "unknown"

# ğŸ“ backend/app/middleware/request_logging.py
import time
import logging
from fastapi import Request, Response
from starlette.middleware.base import BaseHTTPMiddleware

logger = logging.getLogger(__name__)

class RequestLoggingMiddleware(BaseHTTPMiddleware):
    """Request/Response ë¡œê¹… ë¯¸ë“¤ì›¨ì–´"""
    
    async def dispatch(self, request: Request, call_next):
        # Request ID ìƒì„±
        request_id = str(uuid.uuid4())[:8]
        request_id_ctx.set(request_id)
        
        start_time = time.time()
        
        # Request ë¡œê¹…
        logger.info(
            f"[{request_id}] {request.method} {request.url.path} "
            f"from {request.client.host if request.client else 'unknown'}"
        )
        
        try:
            response = await call_next(request)
            
            # Response ë¡œê¹…
            process_time = time.time() - start_time
            logger.info(
                f"[{request_id}] Response: {response.status_code} "
                f"in {process_time:.3f}s"
            )
            
            # ëŠë¦° ìš”ì²­ ê²½ê³ 
            if process_time > 5.0:
                logger.warning(
                    f"[{request_id}] Slow request: {request.method} {request.url.path} "
                    f"took {process_time:.3f}s"
                )
            
            return response
            
        except Exception as e:
            process_time = time.time() - start_time
            logger.error(
                f"[{request_id}] Request failed: {request.method} {request.url.path} "
                f"in {process_time:.3f}s - {str(e)}"
            )
            raise
```

---

## ğŸ“Š Wave 2 êµ¬í˜„ ìš°ì„ ìˆœìœ„

| ì»´í¬ë„ŒíŠ¸ | ìš°ì„ ìˆœìœ„ | ì˜ˆìƒ ì‹œê°„ | ì˜í–¥ë„ |
|---------|---------|---------|--------|
| LLMPermissionsService | Critical | 8h | Critical |
| Circuit Breaker | High | 4h | High |
| ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” | High | 6h | High |
| Request Logging | Medium | 3h | Medium |
| API ì—”ë“œí¬ì¸íŠ¸ ê°œì„  | High | 6h | High |

### Wave 2 ì™„ë£Œ ê¸°ì¤€
- âœ… íŠ¹ì • ëª¨ë¸ IDì— ëŒ€í•œ 500 ì—ëŸ¬ í•´ê²°
- âœ… ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ íƒ€ì„ì•„ì›ƒ ì œê±°
- âœ… Circuit breaker íŒ¨í„´ìœ¼ë¡œ ì¥ì•  ê²©ë¦¬
- âœ… ìƒì„¸í•œ ì—ëŸ¬ ë¡œê¹… ë° Request tracking
- âœ… ë°ì´í„°ë² ì´ìŠ¤ ì¸ë±ìŠ¤ ìµœì í™” ì™„ë£Œ

---

## âš¡ Wave 3: API ì¼ê´€ì„± ë° ì„±ëŠ¥ ê°œì„ 

### 3.1 í˜„ì¬ ë¬¸ì œì  ë¶„ì„

**API ì¼ê´€ì„± ë¶€ì¡±:**
- ë‹¤ì–‘í•œ ì‘ë‹µ í˜•ì‹ê³¼ ì—ëŸ¬ ì²˜ë¦¬ ë°©ì‹
- ìºì‹± ë©”ì»¤ë‹ˆì¦˜ ë¶€ì¬ë¡œ ì¸í•œ ì¤‘ë³µ ì¿¼ë¦¬
- ëŒ€ëŸ‰ ê¶Œí•œ ì²˜ë¦¬ ì‹œ N+1 ë¬¸ì œ ë°œìƒ
- API ë²„ì „ ê´€ë¦¬ ë¶€ì¬

### 3.2 ì„¤ê³„ ì†”ë£¨ì…˜: í‘œì¤€í™”ëœ API ì•„í‚¤í…ì²˜

```python
# ğŸ“ backend/app/core/api_response.py
from typing import Generic, TypeVar, Optional, Any, Dict, List
from pydantic import BaseModel
from datetime import datetime
import uuid

T = TypeVar('T')

class APIResponse(BaseModel, Generic[T]):
    """í‘œì¤€í™”ëœ API ì‘ë‹µ í¬ë§·"""
    success: bool
    data: Optional[T] = None
    error: Optional[str] = None
    error_code: Optional[str] = None
    message: Optional[str] = None
    request_id: Optional[str] = None
    timestamp: datetime
    pagination: Optional['PaginationInfo'] = None
    
    class Config:
        json_encoders = {
            datetime: lambda v: v.isoformat()
        }

class PaginationInfo(BaseModel):
    """í˜ì´ì§€ë„¤ì´ì…˜ ì •ë³´"""
    page: int
    per_page: int
    total: int
    total_pages: int
    has_next: bool
    has_prev: bool

class ErrorDetail(BaseModel):
    """ìƒì„¸ ì—ëŸ¬ ì •ë³´"""
    field: Optional[str] = None
    message: str
    code: str

def success_response(
    data: T, 
    message: str = None,
    request_id: str = None,
    pagination: PaginationInfo = None
) -> APIResponse[T]:
    """ì„±ê³µ ì‘ë‹µ ìƒì„±"""
    return APIResponse(
        success=True,
        data=data,
        message=message,
        request_id=request_id,
        timestamp=datetime.utcnow(),
        pagination=pagination
    )

def error_response(
    error: str,
    error_code: str = None,
    request_id: str = None,
    status_code: int = 500
) -> APIResponse[None]:
    """ì—ëŸ¬ ì‘ë‹µ ìƒì„±"""
    return APIResponse(
        success=False,
        error=error,
        error_code=error_code,
        request_id=request_id,
        timestamp=datetime.utcnow()
    )
```

### 3.3 ìºì‹± ì „ëµ ì„¤ê³„

```python
# ğŸ“ backend/app/core/cache_manager.py
import redis
import json
import hashlib
from typing import Any, Optional, Callable, Dict
from functools import wraps
import asyncio
import logging

logger = logging.getLogger(__name__)

class CacheManager:
    """Redis ê¸°ë°˜ ìºì‹œ ë§¤ë‹ˆì €"""
    
    def __init__(self, redis_url: str = "redis://localhost:6379"):
        self.redis_client = redis.from_url(redis_url, decode_responses=True)
        self.default_ttl = 300  # 5ë¶„
        
    def _generate_cache_key(self, namespace: str, **kwargs) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        key_data = json.dumps(kwargs, sort_keys=True)
        key_hash = hashlib.md5(key_data.encode()).hexdigest()
        return f"{namespace}:{key_hash}"
    
    async def get(self, key: str) -> Optional[Any]:
        """ìºì‹œì—ì„œ ë°ì´í„° ì¡°íšŒ"""
        try:
            cached_data = self.redis_client.get(key)
            if cached_data:
                return json.loads(cached_data)
            return None
        except Exception as e:
            logger.warning(f"Cache get error for key {key}: {e}")
            return None
    
    async def set(self, key: str, value: Any, ttl: int = None) -> bool:
        """ìºì‹œì— ë°ì´í„° ì €ì¥"""
        try:
            serialized_value = json.dumps(value, default=str)
            return self.redis_client.setex(
                key, 
                ttl or self.default_ttl, 
                serialized_value
            )
        except Exception as e:
            logger.warning(f"Cache set error for key {key}: {e}")
            return False
    
    async def delete(self, pattern: str) -> int:
        """íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ìºì‹œ ì‚­ì œ"""
        try:
            keys = self.redis_client.keys(pattern)
            if keys:
                return self.redis_client.delete(*keys)
            return 0
        except Exception as e:
            logger.warning(f"Cache delete error for pattern {pattern}: {e}")
            return 0
    
    def cached(self, namespace: str, ttl: int = None):
        """ìºì‹± ë°ì½”ë ˆì´í„°"""
        def decorator(func: Callable):
            @wraps(func)
            async def wrapper(*args, **kwargs):
                # ìºì‹œ í‚¤ ìƒì„±
                cache_key = self._generate_cache_key(
                    namespace, 
                    args=args, 
                    kwargs=kwargs
                )
                
                # ìºì‹œ í™•ì¸
                cached_result = await self.get(cache_key)
                if cached_result is not None:
                    logger.debug(f"Cache hit for {cache_key}")
                    return cached_result
                
                # í•¨ìˆ˜ ì‹¤í–‰
                result = await func(*args, **kwargs)
                
                # ê²°ê³¼ ìºì‹±
                await self.set(cache_key, result, ttl)
                logger.debug(f"Cache set for {cache_key}")
                
                return result
            return wrapper
        return decorator

# ì „ì—­ ìºì‹œ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
cache_manager = CacheManager()
```

### 3.4 ê°œì„ ëœ LLM ê¶Œí•œ API

```python
# ğŸ“ backend/app/api/llm_models_v2.py
from fastapi import APIRouter, Depends, Query, HTTPException
from typing import List, Optional
from ..core.api_response import APIResponse, success_response, error_response, PaginationInfo
from ..core.cache_manager import cache_manager
from ..services.llm_permissions_service import LLMPermissionsService
import asyncio

router = APIRouter(prefix="/api/v2/llm-models", tags=["LLM Models V2"])

@router.get("/", response_model=APIResponse[List[ModelResponse]])
async def get_models_v2(
    page: int = Query(1, ge=1, description="í˜ì´ì§€ ë²ˆí˜¸"),
    per_page: int = Query(20, ge=1, le=100, description="í˜ì´ì§€ë‹¹ í•­ëª© ìˆ˜"),
    accessible_only: bool = Query(False, description="ì ‘ê·¼ ê°€ëŠ¥í•œ ëª¨ë¸ë§Œ ì¡°íšŒ"),
    search: Optional[str] = Query(None, description="ëª¨ë¸ëª… ê²€ìƒ‰"),
    current_user: User = Depends(get_current_user),
    request_id: str = Depends(get_request_context)
):
    """ê°œì„ ëœ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ API"""
    try:
        # ìºì‹œ í‚¤ ìƒì„±ì„ ìœ„í•œ íŒŒë¼ë¯¸í„°
        cache_params = {
            "user_id": str(current_user.id),
            "page": page,
            "per_page": per_page,
            "accessible_only": accessible_only,
            "search": search or "",
            "is_superuser": current_user.is_superuser
        }
        
        @cache_manager.cached("models_list", ttl=180)  # 3ë¶„ ìºì‹±
        async def _get_models_cached(**params):
            return await permissions_service.get_models_paginated(
                user_id=params["user_id"],
                page=params["page"],
                per_page=params["per_page"],
                accessible_only=params["accessible_only"],
                search=params.get("search"),
                is_superuser=params["is_superuser"]
            )
        
        # ìºì‹œëœ ê²°ê³¼ ì¡°íšŒ
        result = await _get_models_cached(**cache_params)
        
        # í˜ì´ì§€ë„¤ì´ì…˜ ì •ë³´ êµ¬ì„±
        pagination = PaginationInfo(
            page=page,
            per_page=per_page,
            total=result["total"],
            total_pages=(result["total"] + per_page - 1) // per_page,
            has_next=page * per_page < result["total"],
            has_prev=page > 1
        )
        
        return success_response(
            data=result["models"],
            message=f"{len(result['models'])}ê°œì˜ ëª¨ë¸ì„ ì¡°íšŒí–ˆìŠµë‹ˆë‹¤.",
            request_id=request_id,
            pagination=pagination
        )
        
    except Exception as e:
        logger.error(f"[{request_id}] Error getting models: {str(e)}", exc_info=True)
        return error_response(
            error="ëª¨ë¸ ëª©ë¡ ì¡°íšŒì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
            error_code="MODELS_LIST_ERROR",
            request_id=request_id
        )

@router.get("/{model_id}/permissions", response_model=APIResponse[List[ModelPermissionResponse]])
async def get_model_permissions_v2(
    model_id: str,
    current_user: User = Depends(get_current_user),
    request_id: str = Depends(get_request_context)
):
    """ê°œì„ ëœ ëª¨ë¸ ê¶Œí•œ ì¡°íšŒ API (ìºì‹± ì ìš©)"""
    try:
        @cache_manager.cached("model_permissions", ttl=120)  # 2ë¶„ ìºì‹±
        async def _get_permissions_cached(mid: str, uid: str):
            return await permissions_service.get_model_permissions_with_circuit_breaker(
                model_id=mid
            )
        
        # ê¶Œí•œ í™•ì¸ ë° ìºì‹œëœ ê²°ê³¼ ì¡°íšŒ
        permissions = await _get_permissions_cached(model_id, str(current_user.id))
        
        return success_response(
            data=permissions,
            message=f"{len(permissions)}ê°œì˜ ê¶Œí•œì„ ì¡°íšŒí–ˆìŠµë‹ˆë‹¤.",
            request_id=request_id
        )
        
    except HTTPException as e:
        return error_response(
            error=e.detail,
            error_code="PERMISSION_ERROR",
            request_id=request_id
        )
    except Exception as e:
        logger.error(f"[{request_id}] Error getting permissions: {str(e)}", exc_info=True)
        return error_response(
            error="ê¶Œí•œ ì¡°íšŒì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
            error_code="PERMISSIONS_GET_ERROR",
            request_id=request_id
        )

@router.post("/batch-permissions", response_model=APIResponse[Dict[str, Any]])
async def batch_grant_permissions(
    batch_request: BatchPermissionRequest,
    current_user: User = Depends(get_current_user),
    request_id: str = Depends(get_request_context)
):
    """ëŒ€ëŸ‰ ê¶Œí•œ ë¶€ì—¬ API"""
    try:
        results = {
            "success": [],
            "failed": [],
            "total": len(batch_request.permissions)
        }
        
        # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì„±ëŠ¥ í–¥ìƒ
        semaphore = asyncio.Semaphore(5)  # ë™ì‹œ ì²˜ë¦¬ ì œí•œ
        
        async def grant_single_permission(perm_req: PermissionRequest):
            async with semaphore:
                try:
                    result = await permissions_service.grant_permission_with_validation(
                        model_id=perm_req.model_id,
                        grantee_type=perm_req.grantee_type,
                        grantee_id=perm_req.grantee_id,
                        granted_by=str(current_user.id)
                    )
                    results["success"].append({
                        "model_id": perm_req.model_id,
                        "grantee_id": perm_req.grantee_id,
                        "permission_id": result["id"]
                    })
                except Exception as e:
                    results["failed"].append({
                        "model_id": perm_req.model_id,
                        "grantee_id": perm_req.grantee_id,
                        "error": str(e)
                    })
        
        # ë³‘ë ¬ ì‹¤í–‰
        await asyncio.gather(*[
            grant_single_permission(perm) 
            for perm in batch_request.permissions
        ])
        
        # ê´€ë ¨ ìºì‹œ ë¬´íš¨í™”
        await cache_manager.delete("model_permissions:*")
        await cache_manager.delete("models_list:*")
        
        success_count = len(results["success"])
        total_count = results["total"]
        
        return success_response(
            data=results,
            message=f"{success_count}/{total_count}ê°œì˜ ê¶Œí•œì´ ì„±ê³µì ìœ¼ë¡œ ë¶€ì—¬ë˜ì—ˆìŠµë‹ˆë‹¤.",
            request_id=request_id
        )
        
    except Exception as e:
        logger.error(f"[{request_id}] Batch permission error: {str(e)}", exc_info=True)
        return error_response(
            error="ëŒ€ëŸ‰ ê¶Œí•œ ë¶€ì—¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
            error_code="BATCH_PERMISSION_ERROR",
            request_id=request_id
        )
```

### 3.5 Rate Limiting ë° Performance ë¯¸ë“¤ì›¨ì–´

```python
# ğŸ“ backend/app/middleware/rate_limiting.py
import time
import redis
from fastapi import Request, HTTPException, status
from starlette.middleware.base import BaseHTTPMiddleware
from typing import Dict, Optional
import logging

logger = logging.getLogger(__name__)

class RateLimitingMiddleware(BaseHTTPMiddleware):
    """Rate limiting ë¯¸ë“¤ì›¨ì–´"""
    
    def __init__(self, app, redis_url: str = "redis://localhost:6379"):
        super().__init__(app)
        self.redis_client = redis.from_url(redis_url, decode_responses=True)
        
        # APIë³„ Rate limit ì„¤ì •
        self.rate_limits = {
            "/api/v2/llm-models/": {"requests": 100, "window": 60},  # 1ë¶„ì— 100íšŒ
            "/api/v2/llm-models/batch-permissions": {"requests": 10, "window": 60},  # 1ë¶„ì— 10íšŒ
            "default": {"requests": 1000, "window": 60}  # ê¸°ë³¸ê°’
        }
    
    async def dispatch(self, request: Request, call_next):
        # Rate limiting í™•ì¸
        if not await self._check_rate_limit(request):
            raise HTTPException(
                status_code=status.HTTP_429_TOO_MANY_REQUESTS,
                detail="Rate limit exceeded. Please try again later."
            )
        
        response = await call_next(request)
        return response
    
    async def _check_rate_limit(self, request: Request) -> bool:
        """Rate limit í™•ì¸"""
        try:
            # í´ë¼ì´ì–¸íŠ¸ ì‹ë³„ (IP + User ID)
            client_ip = request.client.host if request.client else "unknown"
            user_id = getattr(request.state, 'user_id', 'anonymous')
            client_key = f"{client_ip}:{user_id}"
            
            # API ê²½ë¡œì— ë”°ë¥¸ ì œí•œ ì„¤ì •
            path = request.url.path
            limit_config = self._get_limit_config(path)
            
            # Redis í‚¤ ìƒì„±
            redis_key = f"rate_limit:{client_key}:{path}"
            current_time = int(time.time())
            window_start = current_time - limit_config["window"]
            
            # í˜„ì¬ ìš”ì²­ ìˆ˜ í™•ì¸
            pipeline = self.redis_client.pipeline()
            pipeline.zremrangebyscore(redis_key, 0, window_start)
            pipeline.zcard(redis_key)
            pipeline.zadd(redis_key, {str(current_time): current_time})
            pipeline.expire(redis_key, limit_config["window"])
            
            results = pipeline.execute()
            current_requests = results[1]
            
            return current_requests < limit_config["requests"]
            
        except Exception as e:
            logger.warning(f"Rate limiting error: {e}")
            return True  # ì—ëŸ¬ ì‹œì—ëŠ” í—ˆìš©
    
    def _get_limit_config(self, path: str) -> Dict[str, int]:
        """ê²½ë¡œë³„ ì œí•œ ì„¤ì • ì¡°íšŒ"""
        for pattern, config in self.rate_limits.items():
            if pattern != "default" and path.startswith(pattern):
                return config
        return self.rate_limits["default"]

# ğŸ“ backend/app/middleware/performance.py
import time
import psutil
from fastapi import Request
from starlette.middleware.base import BaseHTTPMiddleware
import logging

logger = logging.getLogger(__name__)

class PerformanceMiddleware(BaseHTTPMiddleware):
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë¯¸ë“¤ì›¨ì–´"""
    
    async def dispatch(self, request: Request, call_next):
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        response = await call_next(request)
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        duration = time.time() - start_time
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        memory_used = end_memory - start_memory
        
        # ì„±ëŠ¥ ë¡œê¹…
        if duration > 1.0:  # 1ì´ˆ ì´ìƒ ì†Œìš”ëœ ìš”ì²­
            logger.warning(
                f"Slow request: {request.method} {request.url.path} "
                f"took {duration:.3f}s, memory: {memory_used:.2f}MB"
            )
        
        # ì‘ë‹µ í—¤ë”ì— ì„±ëŠ¥ ì •ë³´ ì¶”ê°€
        response.headers["X-Process-Time"] = str(duration)
        response.headers["X-Memory-Used"] = str(memory_used)
        
        return response
```

### 3.6 API ë²„ì „ ê´€ë¦¬

```python
# ğŸ“ backend/app/main.py (ìˆ˜ì •ëœ ë¶€ë¶„)
from fastapi import FastAPI
from .api import llm_models  # ê¸°ì¡´ v1
from .api import llm_models_v2  # ìƒˆë¡œìš´ v2
from .middleware.rate_limiting import RateLimitingMiddleware
from .middleware.performance import PerformanceMiddleware

app = FastAPI(
    title="MAX Platform API",
    version="2.0.0",
    description="LLM ê¶Œí•œ ê´€ë¦¬ ì‹œìŠ¤í…œ API"
)

# ë¯¸ë“¤ì›¨ì–´ ë“±ë¡
app.add_middleware(PerformanceMiddleware)
app.add_middleware(RateLimitingMiddleware)

# API ë¼ìš°í„° ë“±ë¡
app.include_router(llm_models.router, tags=["LLM Models V1 (Deprecated)"])
app.include_router(llm_models_v2.router, tags=["LLM Models V2"])

@app.get("/api/health")
async def health_check():
    """API ìƒíƒœ í™•ì¸"""
    return {
        "status": "healthy",
        "version": "2.0.0",
        "timestamp": datetime.utcnow().isoformat()
    }

@app.get("/api/version")
async def api_version():
    """API ë²„ì „ ì •ë³´"""
    return {
        "v1": {
            "status": "deprecated",
            "sunset_date": "2024-12-31",
            "description": "Legacy API, use v2 instead"
        },
        "v2": {
            "status": "current",
            "description": "Current stable API with caching and improved error handling"
        }
    }
```

### 3.7 Frontend API í´ë¼ì´ì–¸íŠ¸ ê°œì„ 

```typescript
// ğŸ“ frontend/src/services/apiClientV2.ts
interface APIResponseV2<T> {
  success: boolean;
  data?: T;
  error?: string;
  error_code?: string;
  message?: string;
  request_id?: string;
  timestamp: string;
  pagination?: PaginationInfo;
}

class APIClientV2 {
  private baseURL = '/api/v2';
  private tokenManager: TokenManager;
  private requestQueue: Map<string, Promise<any>> = new Map();

  constructor() {
    this.tokenManager = new TokenManager();
  }

  async request<T>(
    endpoint: string,
    options: RequestOptions = {}
  ): Promise<APIResponseV2<T>> {
    const { method = 'GET', data, headers = {}, cache = false } = options;
    
    // ìš”ì²­ ì¤‘ë³µ ì œê±° (ë™ì¼í•œ GET ìš”ì²­)
    if (method === 'GET' && cache) {
      const cacheKey = `${endpoint}_${JSON.stringify(options)}`;
      if (this.requestQueue.has(cacheKey)) {
        return this.requestQueue.get(cacheKey);
      }
    }

    const token = await this.tokenManager.getValidToken();
    if (token) {
      headers['Authorization'] = `Bearer ${token}`;
    }

    const requestPromise = this._executeRequest<T>(endpoint, {
      method,
      headers: {
        'Content-Type': 'application/json',
        ...headers
      },
      body: data ? JSON.stringify(data) : undefined
    });

    // GET ìš”ì²­ ìºì‹±
    if (method === 'GET' && cache) {
      const cacheKey = `${endpoint}_${JSON.stringify(options)}`;
      this.requestQueue.set(cacheKey, requestPromise);
      
      // 5ì´ˆ í›„ ìºì‹œ ì œê±°
      setTimeout(() => {
        this.requestQueue.delete(cacheKey);
      }, 5000);
    }

    return requestPromise;
  }

  private async _executeRequest<T>(
    endpoint: string,
    fetchOptions: RequestInit
  ): Promise<APIResponseV2<T>> {
    try {
      const response = await fetch(`${this.baseURL}${endpoint}`, fetchOptions);
      const responseData: APIResponseV2<T> = await response.json();

      if (!response.ok) {
        throw new APIError(
          response.status,
          responseData.error || 'Unknown error',
          responseData.error_code
        );
      }

      return responseData;

    } catch (error) {
      if (error instanceof APIError) {
        throw error;
      }
      
      return {
        success: false,
        error: 'Network error occurred',
        error_code: 'NETWORK_ERROR',
        timestamp: new Date().toISOString()
      };
    }
  }

  // ê°œì„ ëœ LLM ëª¨ë¸ ê´€ë ¨ ë©”ì„œë“œë“¤
  async getLLMModels(options: {
    page?: number;
    per_page?: number;
    accessible_only?: boolean;
    search?: string;
  } = {}) {
    const params = new URLSearchParams();
    Object.entries(options).forEach(([key, value]) => {
      if (value !== undefined) {
        params.append(key, String(value));
      }
    });

    return this.request<LLMModel[]>(
      `/llm-models?${params.toString()}`,
      { cache: true }
    );
  }

  async getLLMModelPermissions(modelId: string) {
    return this.request<ModelPermission[]>(
      `/llm-models/${modelId}/permissions`,
      { cache: true }
    );
  }

  async batchGrantPermissions(permissions: PermissionRequest[]) {
    return this.request<BatchPermissionResult>(
      '/llm-models/batch-permissions',
      {
        method: 'POST',
        data: { permissions }
      }
    );
  }
}

// ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
export const apiClientV2 = new APIClientV2();
```

---

## ğŸ“Š Wave 3 êµ¬í˜„ ìš°ì„ ìˆœìœ„

| ì»´í¬ë„ŒíŠ¸ | ìš°ì„ ìˆœìœ„ | ì˜ˆìƒ ì‹œê°„ | ì˜í–¥ë„ |
|---------|---------|---------|--------|
| í‘œì¤€í™”ëœ API Response | High | 4h | High |
| ìºì‹± ì‹œìŠ¤í…œ | High | 6h | High |
| Rate Limiting | Medium | 3h | Medium |
| ëŒ€ëŸ‰ ê¶Œí•œ ì²˜ë¦¬ API | High | 5h | High |
| Frontend API Client V2 | High | 4h | High |
| ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ | Medium | 2h | Medium |

### Wave 3 ì™„ë£Œ ê¸°ì¤€
- âœ… ëª¨ë“  API ì‘ë‹µì´ í‘œì¤€í™”ëœ í¬ë§· ì‚¬ìš©
- âœ… Redis ìºì‹±ìœ¼ë¡œ ì‘ë‹µ ì‹œê°„ 50% ë‹¨ì¶•
- âœ… Rate limitingìœ¼ë¡œ ì„œë²„ ì•ˆì •ì„± í™•ë³´
- âœ… ëŒ€ëŸ‰ ê¶Œí•œ ì²˜ë¦¬ ì‹œ ë³‘ë ¬ ì²˜ë¦¬ ì ìš©
- âœ… API ë²„ì „ ê´€ë¦¬ ì²´ê³„ êµ¬ì¶•

---

## ğŸ“Š Wave 4: ëª¨ë‹ˆí„°ë§ ë° ë””ë²„ê¹… ê¸°ëŠ¥

### 4.1 í˜„ì¬ ë¬¸ì œì  ë¶„ì„

**ëª¨ë‹ˆí„°ë§ ë¶€ì¬:**
- ì‹œìŠ¤í…œ ìƒíƒœ ê°€ì‹œì„± ë¶€ì¡±
- ì„±ëŠ¥ ë³‘ëª© ì§€ì  ì‹ë³„ ì–´ë ¤ì›€
- ì—ëŸ¬ íŒ¨í„´ ì¶”ì  ë¶ˆê°€ëŠ¥
- ì‹¤ì‹œê°„ ì•Œë¦¼ ì²´ê³„ ë¶€ì¬

### 4.2 ì„¤ê³„ ì†”ë£¨ì…˜: í†µí•© ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ

```python
# ğŸ“ backend/app/monitoring/health_checker.py
import psutil
import asyncio
import redis
from sqlalchemy import text
from typing import Dict, Any, List
from datetime import datetime
from ..database import get_db
from ..models.llm_chat import MAXLLM_Model

class HealthChecker:
    """ì‹œìŠ¤í…œ ìƒíƒœ ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self):
        self.redis_client = redis.from_url("redis://localhost:6379")
        
    async def get_system_health(self) -> Dict[str, Any]:
        """ì „ì²´ ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"""
        checks = await asyncio.gather(
            self._check_database(),
            self._check_redis(),
            self._check_system_resources(),
            self._check_api_endpoints(),
            return_exceptions=True
        )
        
        return {
            "timestamp": datetime.utcnow().isoformat(),
            "overall_status": self._calculate_overall_status(checks),
            "components": {
                "database": checks[0] if not isinstance(checks[0], Exception) else self._error_result(checks[0]),
                "redis": checks[1] if not isinstance(checks[1], Exception) else self._error_result(checks[1]),
                "system": checks[2] if not isinstance(checks[2], Exception) else self._error_result(checks[2]),
                "api": checks[3] if not isinstance(checks[3], Exception) else self._error_result(checks[3])
            }
        }
    
    async def _check_database(self) -> Dict[str, Any]:
        """ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ í™•ì¸"""
        try:
            db = next(get_db())
            start_time = time.time()
            
            # ì—°ê²° í…ŒìŠ¤íŠ¸
            result = db.execute(text("SELECT 1"))
            connection_time = time.time() - start_time
            
            # ê¶Œí•œ í…Œì´ë¸” í†µê³„
            permission_count = db.execute(
                text("SELECT COUNT(*) FROM maxllm_model_permissions")
            ).scalar()
            
            model_count = db.execute(
                text("SELECT COUNT(*) FROM maxllm_models WHERE is_active = true")
            ).scalar()
            
            # ëŠë¦° ì¿¼ë¦¬ ì²´í¬
            slow_queries = self._check_slow_queries(db)
            
            return {
                "status": "healthy" if connection_time < 1.0 else "degraded",
                "connection_time_ms": round(connection_time * 1000, 2),
                "metrics": {
                    "active_models": model_count,
                    "total_permissions": permission_count,
                    "slow_queries_count": len(slow_queries)
                },
                "slow_queries": slow_queries[:5]  # ìƒìœ„ 5ê°œë§Œ
            }
            
        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e)
            }
    
    async def _check_redis(self) -> Dict[str, Any]:
        """Redis ìƒíƒœ í™•ì¸"""
        try:
            start_time = time.time()
            self.redis_client.ping()
            ping_time = time.time() - start_time
            
            info = self.redis_client.info()
            memory_usage = info.get('used_memory_human', 'N/A')
            connected_clients = info.get('connected_clients', 0)
            
            # ìºì‹œ íˆíŠ¸ìœ¨ ê³„ì‚°
            cache_hits = info.get('keyspace_hits', 0)
            cache_misses = info.get('keyspace_misses', 0)
            hit_rate = cache_hits / (cache_hits + cache_misses) if (cache_hits + cache_misses) > 0 else 0
            
            return {
                "status": "healthy" if ping_time < 0.1 else "degraded",
                "ping_time_ms": round(ping_time * 1000, 2),
                "metrics": {
                    "memory_usage": memory_usage,
                    "connected_clients": connected_clients,
                    "cache_hit_rate": round(hit_rate * 100, 2)
                }
            }
            
        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e)
            }
    
    async def _check_system_resources(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ìƒíƒœ í™•ì¸"""
        try:
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            disk = psutil.disk_usage('/')
            
            # ì„ê³„ê°’ ì²´í¬
            cpu_status = "healthy" if cpu_percent < 80 else "degraded" if cpu_percent < 95 else "critical"
            memory_status = "healthy" if memory.percent < 80 else "degraded" if memory.percent < 95 else "critical"
            disk_status = "healthy" if disk.percent < 80 else "degraded" if disk.percent < 95 else "critical"
            
            overall_status = max([cpu_status, memory_status, disk_status], 
                                key=lambda x: ["healthy", "degraded", "critical"].index(x))
            
            return {
                "status": overall_status,
                "metrics": {
                    "cpu_percent": round(cpu_percent, 1),
                    "memory_percent": round(memory.percent, 1),
                    "memory_available_gb": round(memory.available / (1024**3), 2),
                    "disk_percent": round(disk.percent, 1),
                    "disk_free_gb": round(disk.free / (1024**3), 2)
                }
            }
            
        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e)
            }
    
    async def _check_api_endpoints(self) -> Dict[str, Any]:
        """í•µì‹¬ API ì—”ë“œí¬ì¸íŠ¸ ìƒíƒœ í™•ì¸"""
        import aiohttp
        
        endpoints = [
            {"path": "/api/health", "timeout": 5},
            {"path": "/api/v2/llm-models/", "timeout": 10, "auth_required": True}
        ]
        
        results = []
        for endpoint in endpoints:
            try:
                start_time = time.time()
                
                headers = {}
                if endpoint.get("auth_required"):
                    # í…ŒìŠ¤íŠ¸ìš© í† í° ìƒì„± ë¡œì§ í•„ìš”
                    headers["Authorization"] = "Bearer test_token"
                
                async with aiohttp.ClientSession() as session:
                    async with session.get(
                        f"http://localhost:8000{endpoint['path']}", 
                        headers=headers,
                        timeout=endpoint["timeout"]
                    ) as response:
                        response_time = time.time() - start_time
                        
                        results.append({
                            "endpoint": endpoint["path"],
                            "status": "healthy" if response.status == 200 else "degraded",
                            "response_time_ms": round(response_time * 1000, 2),
                            "status_code": response.status
                        })
                        
            except Exception as e:
                results.append({
                    "endpoint": endpoint["path"],
                    "status": "unhealthy",
                    "error": str(e)
                })
        
        overall_status = "healthy"
        if any(r["status"] == "unhealthy" for r in results):
            overall_status = "unhealthy"
        elif any(r["status"] == "degraded" for r in results):
            overall_status = "degraded"
        
        return {
            "status": overall_status,
            "endpoints": results
        }
    
    def _check_slow_queries(self, db) -> List[Dict[str, Any]]:
        """ëŠë¦° ì¿¼ë¦¬ ì²´í¬ (PostgreSQL)"""
        try:
            slow_queries = db.execute(text("""
                SELECT query, mean_time, calls, total_time
                FROM pg_stat_statements 
                WHERE mean_time > 1000  -- 1ì´ˆ ì´ìƒ
                ORDER BY mean_time DESC 
                LIMIT 10
            """)).fetchall()
            
            return [
                {
                    "query": row.query[:100] + "..." if len(row.query) > 100 else row.query,
                    "mean_time_ms": round(row.mean_time, 2),
                    "calls": row.calls,
                    "total_time_ms": round(row.total_time, 2)
                }
                for row in slow_queries
            ]
        except Exception:
            return []
    
    def _calculate_overall_status(self, checks: List[Dict[str, Any]]) -> str:
        """ì „ì²´ ìƒíƒœ ê³„ì‚°"""
        statuses = []
        for check in checks:
            if isinstance(check, dict) and "status" in check:
                statuses.append(check["status"])
        
        if "unhealthy" in statuses:
            return "unhealthy"
        elif "critical" in statuses:
            return "critical"
        elif "degraded" in statuses:
            return "degraded"
        else:
            return "healthy"
    
    def _error_result(self, exception: Exception) -> Dict[str, Any]:
        """ì˜ˆì™¸ë¥¼ ì—ëŸ¬ ê²°ê³¼ë¡œ ë³€í™˜"""
        return {
            "status": "unhealthy",
            "error": str(exception)
        }
```

### 4.3 ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸°

```python
# ğŸ“ backend/app/monitoring/metrics_collector.py
import time
import asyncio
from collections import defaultdict, deque
from typing import Dict, Any, Optional
from datetime import datetime, timedelta
import json

class MetricsCollector:
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ì €ì¥"""
    
    def __init__(self, redis_client):
        self.redis_client = redis_client
        self.metrics_buffer = defaultdict(deque)
        self.buffer_size = 1000
        
    async def record_api_call(
        self,
        endpoint: str,
        method: str,
        status_code: int,
        response_time: float,
        user_id: Optional[str] = None
    ):
        """API í˜¸ì¶œ ë©”íŠ¸ë¦­ ê¸°ë¡"""
        timestamp = datetime.utcnow()
        
        metric = {
            "timestamp": timestamp.isoformat(),
            "endpoint": endpoint,
            "method": method,
            "status_code": status_code,
            "response_time": response_time,
            "user_id": user_id
        }
        
        # ë©”ëª¨ë¦¬ ë²„í¼ì— ì„ì‹œ ì €ì¥
        self.metrics_buffer["api_calls"].append(metric)
        
        # ë²„í¼ í¬ê¸° ì œí•œ
        if len(self.metrics_buffer["api_calls"]) > self.buffer_size:
            self.metrics_buffer["api_calls"].popleft()
        
        # Redisì— ì‹¤ì‹œê°„ ì¹´ìš´í„° ì—…ë°ì´íŠ¸
        await self._update_realtime_counters(endpoint, method, status_code, response_time)
    
    async def record_database_query(
        self,
        query_type: str,
        table_name: str,
        execution_time: float,
        affected_rows: int = 0
    ):
        """ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ë©”íŠ¸ë¦­ ê¸°ë¡"""
        timestamp = datetime.utcnow()
        
        metric = {
            "timestamp": timestamp.isoformat(),
            "query_type": query_type,
            "table_name": table_name,
            "execution_time": execution_time,
            "affected_rows": affected_rows
        }
        
        self.metrics_buffer["db_queries"].append(metric)
        
        if len(self.metrics_buffer["db_queries"]) > self.buffer_size:
            self.metrics_buffer["db_queries"].popleft()
    
    async def record_cache_operation(
        self,
        operation: str,  # hit, miss, set, delete
        key_pattern: str,
        execution_time: float
    ):
        """ìºì‹œ ì‘ì—… ë©”íŠ¸ë¦­ ê¸°ë¡"""
        timestamp = datetime.utcnow()
        
        metric = {
            "timestamp": timestamp.isoformat(),
            "operation": operation,
            "key_pattern": key_pattern,
            "execution_time": execution_time
        }
        
        self.metrics_buffer["cache_operations"].append(metric)
        
        if len(self.metrics_buffer["cache_operations"]) > self.buffer_size:
            self.metrics_buffer["cache_operations"].popleft()
    
    async def _update_realtime_counters(
        self,
        endpoint: str,
        method: str,
        status_code: int,
        response_time: float
    ):
        """ì‹¤ì‹œê°„ ì¹´ìš´í„° ì—…ë°ì´íŠ¸"""
        current_minute = datetime.utcnow().strftime("%Y-%m-%d-%H-%M")
        
        pipeline = self.redis_client.pipeline()
        
        # ë¶„ë‹¹ ìš”ì²­ ìˆ˜
        pipeline.incr(f"api_calls:{current_minute}")
        pipeline.expire(f"api_calls:{current_minute}", 3600)  # 1ì‹œê°„ ë³´ì¡´
        
        # ì—”ë“œí¬ì¸íŠ¸ë³„ í†µê³„
        endpoint_key = f"endpoint_stats:{endpoint}:{current_minute}"
        pipeline.incr(f"{endpoint_key}:count")
        pipeline.incrbyfloat(f"{endpoint_key}:total_time", response_time)
        pipeline.expire(f"{endpoint_key}:count", 3600)
        pipeline.expire(f"{endpoint_key}:total_time", 3600)
        
        # ìƒíƒœ ì½”ë“œë³„ í†µê³„
        pipeline.incr(f"status_codes:{status_code}:{current_minute}")
        pipeline.expire(f"status_codes:{status_code}:{current_minute}", 3600)
        
        await pipeline.execute()
    
    async def get_api_statistics(
        self,
        minutes: int = 60
    ) -> Dict[str, Any]:
        """API í†µê³„ ì¡°íšŒ"""
        now = datetime.utcnow()
        stats = {
            "time_range": f"last {minutes} minutes",
            "total_requests": 0,
            "average_response_time": 0,
            "status_code_distribution": defaultdict(int),
            "endpoint_stats": defaultdict(lambda: {"count": 0, "avg_time": 0}),
            "error_rate": 0
        }
        
        # ìµœê·¼ Në¶„ê°„ì˜ ë°ì´í„° ìˆ˜ì§‘
        for i in range(minutes):
            minute_time = now - timedelta(minutes=i)
            minute_key = minute_time.strftime("%Y-%m-%d-%H-%M")
            
            # ì´ ìš”ì²­ ìˆ˜
            total_requests = self.redis_client.get(f"api_calls:{minute_key}")
            if total_requests:
                stats["total_requests"] += int(total_requests)
            
            # ìƒíƒœ ì½”ë“œ ë¶„í¬
            for status_code in [200, 400, 401, 403, 404, 500, 502, 503]:
                count = self.redis_client.get(f"status_codes:{status_code}:{minute_key}")
                if count:
                    stats["status_code_distribution"][status_code] += int(count)
        
        # í‰ê·  ì‘ë‹µ ì‹œê°„ ê³„ì‚°
        total_time = 0
        total_count = 0
        
        for endpoint_data in self.metrics_buffer["api_calls"]:
            if datetime.fromisoformat(endpoint_data["timestamp"]) > now - timedelta(minutes=minutes):
                total_time += endpoint_data["response_time"]
                total_count += 1
        
        if total_count > 0:
            stats["average_response_time"] = round(total_time / total_count * 1000, 2)  # ms
        
        # ì—ëŸ¬ìœ¨ ê³„ì‚°
        error_count = sum(
            count for status, count in stats["status_code_distribution"].items()
            if status >= 400
        )
        if stats["total_requests"] > 0:
            stats["error_rate"] = round(error_count / stats["total_requests"] * 100, 2)
        
        return dict(stats)
    
    async def flush_metrics_to_storage(self):
        """ë©”íŠ¸ë¦­ì„ ì˜êµ¬ ì €ì¥ì†Œì— í”ŒëŸ¬ì‹œ"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” InfluxDB, Prometheus ë“±ì— ì €ì¥
        timestamp = datetime.utcnow()
        
        for metric_type, metrics in self.metrics_buffer.items():
            if metrics:
                # JSONìœ¼ë¡œ ì§ë ¬í™”í•˜ì—¬ Redisì— ì €ì¥ (ì„ì‹œ)
                key = f"metrics_archive:{metric_type}:{timestamp.strftime('%Y-%m-%d-%H')}"
                
                metrics_data = {
                    "timestamp": timestamp.isoformat(),
                    "metric_type": metric_type,
                    "data": list(metrics)
                }
                
                self.redis_client.setex(
                    key,
                    86400,  # 24ì‹œê°„ ë³´ì¡´
                    json.dumps(metrics_data, default=str)
                )
                
                # ë²„í¼ í´ë¦¬ì–´
                metrics.clear()
```

### 4.4 ì•Œë¦¼ ì‹œìŠ¤í…œ

```python
# ğŸ“ backend/app/monitoring/alerting.py
import asyncio
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from typing import List, Dict, Any, Callable
from datetime import datetime, timedelta
from enum import Enum
import logging

logger = logging.getLogger(__name__)

class AlertSeverity(Enum):
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"

class AlertRule:
    """ì•Œë¦¼ ê·œì¹™ ì •ì˜"""
    
    def __init__(
        self,
        name: str,
        condition: Callable[[Dict[str, Any]], bool],
        severity: AlertSeverity,
        message_template: str,
        cooldown_minutes: int = 10
    ):
        self.name = name
        self.condition = condition
        self.severity = severity
        self.message_template = message_template
        self.cooldown_minutes = cooldown_minutes
        self.last_triggered = None

class AlertManager:
    """ì•Œë¦¼ ê´€ë¦¬ì"""
    
    def __init__(self, smtp_config: Dict[str, str], recipients: List[str]):
        self.smtp_config = smtp_config
        self.recipients = recipients
        self.rules = []
        self.alert_history = []
        
    def add_rule(self, rule: AlertRule):
        """ì•Œë¦¼ ê·œì¹™ ì¶”ê°€"""
        self.rules.append(rule)
    
    async def check_and_send_alerts(self, health_data: Dict[str, Any]):
        """ìƒíƒœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì•Œë¦¼ ê·œì¹™ ì²´í¬ ë° ë°œì†¡"""
        triggered_alerts = []
        
        for rule in self.rules:
            try:
                if rule.condition(health_data):
                    # ì¿¨ë‹¤ìš´ ì²´í¬
                    if self._is_in_cooldown(rule):
                        continue
                    
                    alert = {
                        "rule_name": rule.name,
                        "severity": rule.severity.value,
                        "message": rule.message_template.format(**health_data),
                        "timestamp": datetime.utcnow().isoformat(),
                        "data": health_data
                    }
                    
                    triggered_alerts.append(alert)
                    rule.last_triggered = datetime.utcnow()
                    
                    # ì•Œë¦¼ ë°œì†¡
                    await self._send_alert(alert)
                    
            except Exception as e:
                logger.error(f"Error checking alert rule {rule.name}: {e}")
        
        # ì•Œë¦¼ ê¸°ë¡ ì €ì¥
        self.alert_history.extend(triggered_alerts)
        
        # ê¸°ë¡ í¬ê¸° ì œí•œ (ìµœê·¼ 1000ê°œë§Œ ë³´ê´€)
        if len(self.alert_history) > 1000:
            self.alert_history = self.alert_history[-1000:]
        
        return triggered_alerts
    
    def _is_in_cooldown(self, rule: AlertRule) -> bool:
        """ì¿¨ë‹¤ìš´ ê¸°ê°„ í™•ì¸"""
        if not rule.last_triggered:
            return False
        
        cooldown_period = timedelta(minutes=rule.cooldown_minutes)
        return datetime.utcnow() - rule.last_triggered < cooldown_period
    
    async def _send_alert(self, alert: Dict[str, Any]):
        """ì•Œë¦¼ ë°œì†¡"""
        try:
            subject = f"[{alert['severity'].upper()}] MAX Platform Alert: {alert['rule_name']}"
            body = self._format_alert_email(alert)
            
            # ì´ë©”ì¼ ë°œì†¡
            await self._send_email(subject, body)
            
            # Slack ì•Œë¦¼ (êµ¬í˜„ ì‹œ)
            # await self._send_slack_notification(alert)
            
            logger.info(f"Alert sent: {alert['rule_name']}")
            
        except Exception as e:
            logger.error(f"Failed to send alert: {e}")
    
    async def _send_email(self, subject: str, body: str):
        """ì´ë©”ì¼ ë°œì†¡"""
        try:
            msg = MIMEMultipart()
            msg['From'] = self.smtp_config['from']
            msg['To'] = ', '.join(self.recipients)
            msg['Subject'] = subject
            
            msg.attach(MIMEText(body, 'html'))
            
            server = smtplib.SMTP(self.smtp_config['host'], self.smtp_config['port'])
            if self.smtp_config.get('use_tls'):
                server.starttls()
            if self.smtp_config.get('username'):
                server.login(self.smtp_config['username'], self.smtp_config['password'])
            
            server.send_message(msg)
            server.quit()
            
        except Exception as e:
            logger.error(f"Failed to send email: {e}")
    
    def _format_alert_email(self, alert: Dict[str, Any]) -> str:
        """ì•Œë¦¼ ì´ë©”ì¼ í¬ë§·íŒ…"""
        severity_colors = {
            "info": "#17a2b8",
            "warning": "#ffc107", 
            "error": "#dc3545",
            "critical": "#721c24"
        }
        
        color = severity_colors.get(alert['severity'], "#6c757d")
        
        return f"""
        <html>
        <body>
            <div style="font-family: Arial, sans-serif; max-width: 600px;">
                <div style="background-color: {color}; color: white; padding: 20px; border-radius: 5px 5px 0 0;">
                    <h2 style="margin: 0;">âš ï¸ MAX Platform Alert</h2>
                    <p style="margin: 5px 0 0 0;">Severity: {alert['severity'].upper()}</p>
                </div>
                
                <div style="background-color: #f8f9fa; padding: 20px; border-radius: 0 0 5px 5px;">
                    <h3>Rule: {alert['rule_name']}</h3>
                    <p><strong>Message:</strong> {alert['message']}</p>
                    <p><strong>Time:</strong> {alert['timestamp']}</p>
                    
                    <details>
                        <summary>System Status Details</summary>
                        <pre style="background-color: #e9ecef; padding: 10px; border-radius: 3px; overflow-x: auto;">
{json.dumps(alert['data'], indent=2)}
                        </pre>
                    </details>
                </div>
            </div>
        </body>
        </html>
        """

# ê¸°ë³¸ ì•Œë¦¼ ê·œì¹™ë“¤
def create_default_alert_rules() -> List[AlertRule]:
    """ê¸°ë³¸ ì•Œë¦¼ ê·œì¹™ ìƒì„±"""
    rules = []
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨
    rules.append(AlertRule(
        name="Database Connection Failed",
        condition=lambda data: data.get("components", {}).get("database", {}).get("status") == "unhealthy",
        severity=AlertSeverity.CRITICAL,
        message_template="Database connection failed: {components[database][error]}",
        cooldown_minutes=5
    ))
    
    # ë†’ì€ ì‘ë‹µ ì‹œê°„
    rules.append(AlertRule(
        name="High Response Time",
        condition=lambda data: data.get("components", {}).get("database", {}).get("connection_time_ms", 0) > 5000,
        severity=AlertSeverity.WARNING,
        message_template="Database response time is high: {components[database][connection_time_ms]}ms",
        cooldown_minutes=15
    ))
    
    # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ë¶€ì¡±
    rules.append(AlertRule(
        name="High System Resource Usage",
        condition=lambda data: (
            data.get("components", {}).get("system", {}).get("metrics", {}).get("cpu_percent", 0) > 90 or
            data.get("components", {}).get("system", {}).get("metrics", {}).get("memory_percent", 0) > 90
        ),
        severity=AlertSeverity.ERROR,
        message_template="System resources critical - CPU: {components[system][metrics][cpu_percent]}%, Memory: {components[system][metrics][memory_percent]}%",
        cooldown_minutes=10
    ))
    
    # Redis ì—°ê²° ì‹¤íŒ¨
    rules.append(AlertRule(
        name="Redis Connection Failed",
        condition=lambda data: data.get("components", {}).get("redis", {}).get("status") == "unhealthy",
        severity=AlertSeverity.ERROR,
        message_template="Redis connection failed: {components[redis][error]}",
        cooldown_minutes=5
    ))
    
    return rules
```

### 4.5 ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ API

```python
# ğŸ“ backend/app/api/monitoring.py
from fastapi import APIRouter, Depends, WebSocket, WebSocketDisconnect
from fastapi.responses import HTMLResponse
from typing import List
import asyncio
import json
from ..monitoring.health_checker import HealthChecker
from ..monitoring.metrics_collector import MetricsCollector
from ..monitoring.alerting import AlertManager, create_default_alert_rules
from ..utils.auth import get_current_user

router = APIRouter(prefix="/api/monitoring", tags=["Monitoring"])

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ë“¤
health_checker = HealthChecker()
metrics_collector = MetricsCollector(redis.from_url("redis://localhost:6379"))
alert_manager = AlertManager(
    smtp_config={
        "host": "smtp.gmail.com",
        "port": 587,
        "use_tls": True,
        "username": "alerts@maxplatform.com",
        "password": "app_password"
    },
    recipients=["admin@maxplatform.com"]
)

# ê¸°ë³¸ ì•Œë¦¼ ê·œì¹™ ì¶”ê°€
for rule in create_default_alert_rules():
    alert_manager.add_rule(rule)

@router.get("/health")
async def get_health_status():
    """ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ"""
    health_data = await health_checker.get_system_health()
    
    # ì•Œë¦¼ ì²´í¬ ë° ë°œì†¡
    await alert_manager.check_and_send_alerts(health_data)
    
    return health_data

@router.get("/metrics")
async def get_metrics(
    minutes: int = 60,
    current_user = Depends(get_current_user)
):
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
    if not current_user.is_superuser:
        raise HTTPException(status_code=403, detail="Admin access required")
    
    api_stats = await metrics_collector.get_api_statistics(minutes)
    
    return {
        "time_range_minutes": minutes,
        "api_statistics": api_stats,
        "system_metrics": await health_checker._check_system_resources()
    }

@router.get("/alerts")
async def get_alert_history(
    limit: int = 100,
    current_user = Depends(get_current_user)
):
    """ì•Œë¦¼ ì´ë ¥ ì¡°íšŒ"""
    if not current_user.is_superuser:
        raise HTTPException(status_code=403, detail="Admin access required")
    
    return {
        "alerts": alert_manager.alert_history[-limit:],
        "total_count": len(alert_manager.alert_history)
    }

class ConnectionManager:
    """WebSocket ì—°ê²° ê´€ë¦¬ì"""
    
    def __init__(self):
        self.active_connections: List[WebSocket] = []
    
    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)
    
    def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)
    
    async def broadcast(self, message: dict):
        for connection in self.active_connections:
            try:
                await connection.send_text(json.dumps(message))
            except:
                # ì—°ê²°ì´ ëŠì–´ì§„ ê²½ìš° ì œê±°
                self.active_connections.remove(connection)

connection_manager = ConnectionManager()

@router.websocket("/realtime")
async def websocket_realtime_monitoring(websocket: WebSocket):
    """ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ WebSocket"""
    await connection_manager.connect(websocket)
    
    try:
        while True:
            # 5ì´ˆë§ˆë‹¤ ìƒíƒœ ì •ë³´ ì „ì†¡
            health_data = await health_checker.get_system_health()
            api_stats = await metrics_collector.get_api_statistics(5)  # ìµœê·¼ 5ë¶„
            
            message = {
                "type": "status_update",
                "timestamp": datetime.utcnow().isoformat(),
                "health": health_data,
                "metrics": api_stats
            }
            
            await websocket.send_text(json.dumps(message))
            await asyncio.sleep(5)
            
    except WebSocketDisconnect:
        connection_manager.disconnect(websocket)

@router.get("/dashboard")
async def get_monitoring_dashboard():
    """ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ HTML"""
    return HTMLResponse(content="""
    <!DOCTYPE html>
    <html>
    <head>
        <title>MAX Platform Monitoring</title>
        <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
        <style>
            body { font-family: Arial, sans-serif; margin: 20px; }
            .metric-card { 
                background: #f8f9fa; 
                padding: 20px; 
                margin: 10px; 
                border-radius: 5px; 
                border-left: 4px solid #007bff;
            }
            .status-healthy { border-left-color: #28a745; }
            .status-degraded { border-left-color: #ffc107; }
            .status-unhealthy { border-left-color: #dc3545; }
            .real-time { background: #e3f2fd; }
        </style>
    </head>
    <body>
        <h1>MAX Platform Real-time Monitoring</h1>
        
        <div id="status-overview"></div>
        
        <div class="real-time">
            <h3>ğŸ“Š Real-time Metrics</h3>
            <div id="realtime-data"></div>
        </div>
        
        <canvas id="metricsChart" width="400" height="200"></canvas>
        
        <script>
            const ws = new WebSocket('ws://localhost:8000/api/monitoring/realtime');
            const chartData = {
                labels: [],
                datasets: [{
                    label: 'Response Time (ms)',
                    data: [],
                    borderColor: 'rgb(75, 192, 192)',
                    tension: 0.1
                }]
            };
            
            const chart = new Chart(document.getElementById('metricsChart'), {
                type: 'line',
                data: chartData,
                options: {
                    responsive: true,
                    scales: {
                        y: { beginAtZero: true }
                    }
                }
            });
            
            ws.onmessage = function(event) {
                const data = JSON.parse(event.data);
                
                // ìƒíƒœ ì—…ë°ì´íŠ¸
                document.getElementById('status-overview').innerHTML = `
                    <div class="metric-card status-${data.health.overall_status}">
                        <h3>ğŸš¦ Overall Status: ${data.health.overall_status.toUpperCase()}</h3>
                        <p>Last updated: ${data.timestamp}</p>
                    </div>
                `;
                
                // ì‹¤ì‹œê°„ ë°ì´í„° ì—…ë°ì´íŠ¸
                document.getElementById('realtime-data').innerHTML = `
                    <p>ğŸ“ˆ Total Requests: ${data.metrics.total_requests}</p>
                    <p>â±ï¸ Avg Response Time: ${data.metrics.average_response_time}ms</p>
                    <p>âŒ Error Rate: ${data.metrics.error_rate}%</p>
                `;
                
                // ì°¨íŠ¸ ì—…ë°ì´íŠ¸
                const now = new Date().toLocaleTimeString();
                chartData.labels.push(now);
                chartData.datasets[0].data.push(data.metrics.average_response_time);
                
                // ìµœê·¼ 20ê°œ ë°ì´í„°ë§Œ ìœ ì§€
                if (chartData.labels.length > 20) {
                    chartData.labels.shift();
                    chartData.datasets[0].data.shift();
                }
                
                chart.update();
            };
        </script>
    </body>
    </html>
    """)
```

---

## ğŸ“Š Wave 4 êµ¬í˜„ ìš°ì„ ìˆœìœ„

| ì»´í¬ë„ŒíŠ¸ | ìš°ì„ ìˆœìœ„ | ì˜ˆìƒ ì‹œê°„ | ì˜í–¥ë„ |
|---------|---------|---------|--------|
| Health Checker | High | 6h | High |
| Metrics Collector | High | 5h | High |
| Alert Manager | Medium | 4h | Medium |
| Real-time Dashboard | Medium | 3h | Medium |
| WebSocket ëª¨ë‹ˆí„°ë§ | Low | 2h | Low |

### Wave 4 ì™„ë£Œ ê¸°ì¤€
- âœ… ì‹œìŠ¤í…œ ìƒíƒœ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
- âœ… ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ë¶„ì„
- âœ… ìë™ ì•Œë¦¼ ì‹œìŠ¤í…œ êµ¬ì¶•
- âœ… ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ ì œê³µ
- âœ… ë¬¸ì œ ë°œìƒ ì‹œ ì¦‰ì‹œ ê°ì§€ ë° ì•Œë¦¼

---

## ğŸš€ Wave 5: êµ¬í˜„ ë¡œë“œë§µ ë° ë§ˆì´ê·¸ë ˆì´ì…˜ ì „ëµ

### 5.1 ì „ì²´ êµ¬í˜„ ë¡œë“œë§µ

```mermaid
gantt
    title LLM ê¶Œí•œ ì‹œìŠ¤í…œ ê°œì„  ë¡œë“œë§µ
    dateFormat  YYYY-MM-DD
    section Phase 1: ì¸í”„ë¼
    Database ìµœì í™”       :db, 2024-08-01, 3d
    Redis ì„¤ì •           :redis, after db, 2d
    Circuit Breaker      :cb, after redis, 2d
    section Phase 2: ë°±ì—”ë“œ
    LLM Permissions Service :service, after cb, 4d
    API V2 ê°œë°œ          :api, after service, 3d
    Error Handling       :error, after api, 2d
    section Phase 3: í”„ë¡ íŠ¸ì—”ë“œ
    API Client V2        :client, after error, 2d
    AdminPage ë¦¬íŒ©í† ë§    :admin, after client, 3d
    Token Manager        :token, after admin, 2d
    section Phase 4: ëª¨ë‹ˆí„°ë§
    Health Checker       :health, after token, 2d
    Metrics Collector    :metrics, after health, 2d
    Alert System         :alert, after metrics, 2d
    section Phase 5: ë°°í¬
    í†µí•© í…ŒìŠ¤íŠ¸           :test, after alert, 3d
    í”„ë¡œë•ì…˜ ë°°í¬         :prod, after test, 2d
    ëª¨ë‹ˆí„°ë§ ê²€ì¦         :verify, after prod, 1d
```

### 5.2 ë‹¨ê³„ë³„ êµ¬í˜„ ê³„íš

#### Phase 1: ì¸í”„ë¼ êµ¬ì¶• (1ì£¼ì°¨)

**ëª©í‘œ**: ì•ˆì •ì ì¸ ê¸°ë°˜ ì¸í”„ë¼ êµ¬ì¶•

**ì‘ì—… ë‚´ìš©**:
```bash
# Day 1-3: ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”
- ì¸ë±ìŠ¤ ìƒì„± ë° ìµœì í™”
- ì¿¼ë¦¬ ì„±ëŠ¥ ê°œì„ 
- ì—°ê²° í’€ íŠœë‹

# Day 4-5: Redis ì„¤ì •
- Redis í´ëŸ¬ìŠ¤í„° êµ¬ì¶•
- ìºì‹± ì „ëµ êµ¬í˜„
- ë°±ì—… ë° ë³µêµ¬ ì„¤ì •

# Day 6-7: Circuit Breaker íŒ¨í„´
- ì¥ì•  ê²©ë¦¬ ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„
- ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ ì„¤ì •
```

**ì™„ë£Œ ê¸°ì¤€**:
- âœ… ë°ì´í„°ë² ì´ìŠ¤ ì‘ë‹µ ì‹œê°„ < 100ms
- âœ… Redis ìºì‹œ íˆíŠ¸ìœ¨ > 80%
- âœ… Circuit breaker ì •ìƒ ì‘ë™

**ìœ„í—˜ ìš”ì†Œ**:
- ë°ì´í„°ë² ì´ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜ ì¤‘ ë‹¤ìš´íƒ€ì„
- Redis ë©”ëª¨ë¦¬ ë¶€ì¡±

**ì™„í™” ì „ëµ**:
- Blue-green ë°°í¬ë¡œ ë‹¤ìš´íƒ€ì„ ìµœì†Œí™”
- Redis ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ ì„¤ì •

#### Phase 2: ë°±ì—”ë“œ ê°œë°œ (2ì£¼ì°¨)

**ëª©í‘œ**: ì•ˆì •ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ ë°±ì—”ë“œ API êµ¬ì¶•

**ì‘ì—… ë‚´ìš©**:
```python
# Day 1-4: LLM Permissions Service
- ê¶Œí•œ ê´€ë¦¬ ë¡œì§ ê°œë°œ
- íŠ¸ëœì­ì…˜ ì²˜ë¦¬
- ì„±ëŠ¥ ìµœì í™”

# Day 5-7: API V2 ê°œë°œ  
- RESTful API ì„¤ê³„
- í‘œì¤€í™”ëœ ì‘ë‹µ í¬ë§·
- API ë¬¸ì„œí™”

# Day 8-9: Error Handling
- ì¤‘ì•™í™”ëœ ì—ëŸ¬ ì²˜ë¦¬
- ë¡œê¹… ì‹œìŠ¤í…œ êµ¬ì¶•
- ëª¨ë‹ˆí„°ë§ í†µí•©
```

**í…ŒìŠ¤íŠ¸ ì „ëµ**:
```python
# ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
def test_permission_service():
    service = LLMPermissionsService()
    result = service.grant_permission("model_id", "USER", "user_id")
    assert result["success"] == True

# í†µí•© í…ŒìŠ¤íŠ¸
async def test_api_endpoints():
    async with AsyncClient(app=app, base_url="http://test") as ac:
        response = await ac.get("/api/v2/llm-models/")
        assert response.status_code == 200

# ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
def test_performance():
    # 100ê°œ ë™ì‹œ ìš”ì²­ ì²˜ë¦¬ ê°€ëŠ¥
    # í‰ê·  ì‘ë‹µ ì‹œê°„ < 200ms
    # ì—ëŸ¬ìœ¨ < 0.1%
```

#### Phase 3: í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œ (3ì£¼ì°¨)

**ëª©í‘œ**: ì‚¬ìš©ì ì¹œí™”ì ì´ê³  ì•ˆì •ì ì¸ UI êµ¬ì¶•

**ì‘ì—… ë‚´ìš©**:
```typescript
// Day 1-2: API Client V2
- í†µí•©ëœ API í´ë¼ì´ì–¸íŠ¸
- ìë™ í† í° ê´€ë¦¬
- ì—ëŸ¬ ì²˜ë¦¬ ê°œì„ 

// Day 3-5: AdminPage ë¦¬íŒ©í† ë§
- ì»´í¬ë„ŒíŠ¸ ëª¨ë“ˆí™”
- ìƒíƒœ ê´€ë¦¬ ê°œì„ 
- ì‚¬ìš©ì ê²½í—˜ í–¥ìƒ

// Day 6-7: Token Manager
- ìë™ í† í° ê°±ì‹ 
- ì„¸ì…˜ ê´€ë¦¬
- ë³´ì•ˆ ê°•í™”
```

**UI/UX í…ŒìŠ¤íŠ¸**:
```typescript
// E2E í…ŒìŠ¤íŠ¸
describe('Admin Page LLM Permissions', () => {
  it('should load model permissions successfully', async () => {
    await page.goto('/admin');
    await page.click('[data-testid="llm-models-tab"]');
    
    const permissions = await page.waitForSelector('[data-testid="permissions-list"]');
    expect(permissions).toBeTruthy();
  });
  
  it('should grant permission successfully', async () => {
    // ê¶Œí•œ ë¶€ì—¬ í”Œë¡œìš° í…ŒìŠ¤íŠ¸
  });
});
```

#### Phase 4: ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ (4ì£¼ì°¨)

**ëª©í‘œ**: ì¢…í•©ì ì¸ ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ êµ¬ì¶•

**ì„¤ì¹˜ ìˆœì„œ**:
```bash
# Day 1-2: Health Checker
docker run -d --name redis redis:alpine
python -m pip install psutil aiohttp

# Day 3-4: Metrics Collector  
# InfluxDB ì„¤ì¹˜ (ì„ íƒì‚¬í•­)
docker run -d -p 8086:8086 influxdb:2.0

# Day 5-6: Alert System
# SMTP ì„¤ì •
# Slack ì›¹í›… ì„¤ì • (ì„ íƒì‚¬í•­)
```

**ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ êµ¬ì„±**:
```yaml
# Grafana ëŒ€ì‹œë³´ë“œ (ì„ íƒì‚¬í•­)
panels:
  - title: "API Response Time"
    type: "graph"
    targets:
      - expr: "avg(api_response_time)"
  
  - title: "Error Rate"
    type: "stat"
    targets:
      - expr: "rate(api_errors_total[5m]) * 100"
  
  - title: "Database Connection"
    type: "stat"
    targets:
      - expr: "database_connection_time"
```

#### Phase 5: í†µí•© ë° ë°°í¬ (5ì£¼ì°¨)

**í†µí•© í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤**:
```python
# ì „ì²´ í”Œë¡œìš° í…ŒìŠ¤íŠ¸
async def test_complete_permission_flow():
    # 1. ì‚¬ìš©ì ë¡œê·¸ì¸
    login_response = await client.post("/api/auth/login", json={
        "email": "admin@test.com",
        "password": "password"
    })
    token = login_response.json()["access_token"]
    
    # 2. ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
    models = await client.get("/api/v2/llm-models/", 
                            headers={"Authorization": f"Bearer {token}"})
    assert models.status_code == 200
    
    # 3. ê¶Œí•œ ë¶€ì—¬
    permission = await client.post(f"/api/v2/llm-models/{model_id}/permissions",
                                  json={"grantee_type": "USER", "grantee_id": "user_id"},
                                  headers={"Authorization": f"Bearer {token}"})
    assert permission.status_code == 200
    
    # 4. ê¶Œí•œ ì¡°íšŒ ê²€ì¦
    permissions = await client.get(f"/api/v2/llm-models/{model_id}/permissions",
                                  headers={"Authorization": f"Bearer {token}"})
    assert len(permissions.json()["data"]) > 0
```

### 5.3 ë°°í¬ ì „ëµ

#### Blue-Green ë°°í¬

```yaml
# docker-compose.blue.yml
version: '3.8'
services:
  backend-blue:
    image: maxplatform-backend:v2.0
    environment:
      - DATABASE_URL=postgresql://user:pass@db:5432/maxplatform
      - REDIS_URL=redis://redis:6379
    ports:
      - "8001:8000"
  
  frontend-blue:
    image: maxplatform-frontend:v2.0
    environment:
      - VITE_API_BASE_URL=http://localhost:8001
    ports:
      - "3001:3000"

# docker-compose.green.yml  
version: '3.8'
services:
  backend-green:
    image: maxplatform-backend:v2.0
    environment:
      - DATABASE_URL=postgresql://user:pass@db:5432/maxplatform
      - REDIS_URL=redis://redis:6379
    ports:
      - "8002:8000"
  
  frontend-green:
    image: maxplatform-frontend:v2.0
    environment:
      - VITE_API_BASE_URL=http://localhost:8002
    ports:
      - "3002:3000"
```

#### ë°°í¬ ìŠ¤í¬ë¦½íŠ¸

```bash
#!/bin/bash
# deploy.sh

set -e

ENVIRONMENT=${1:-blue}
VERSION=${2:-latest}

echo "ğŸš€ Deploying to $ENVIRONMENT environment..."

# 1. ì´ë¯¸ì§€ ë¹Œë“œ
docker build -t maxplatform-backend:$VERSION ./backend
docker build -t maxplatform-frontend:$VERSION ./frontend

# 2. ìƒˆ í™˜ê²½ ì‹œì‘
docker-compose -f docker-compose.$ENVIRONMENT.yml up -d

# 3. í—¬ìŠ¤ ì²´í¬
echo "â³ Waiting for services to be ready..."
timeout 60 bash -c 'until curl -f http://localhost:800X/api/health; do sleep 2; done'

# 4. íŠ¸ë˜í”½ ìŠ¤ìœ„ì¹­ (nginx ì„¤ì • ë³€ê²½)
echo "ğŸ”„ Switching traffic to $ENVIRONMENT..."
nginx -s reload

# 5. ì´ì „ í™˜ê²½ ì •ë¦¬
OLD_ENV=$([ "$ENVIRONMENT" = "blue" ] && echo "green" || echo "blue")
docker-compose -f docker-compose.$OLD_ENV.yml down

echo "âœ… Deployment complete!"
```

### 5.4 ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹

#### ëª©í‘œ ì„±ëŠ¥ ì§€í‘œ

```yaml
ì„±ëŠ¥_ëª©í‘œ:
  API_ì‘ë‹µì‹œê°„:
    - ê¶Œí•œ_ì¡°íšŒ: "<200ms (P95)"
    - ê¶Œí•œ_ë¶€ì—¬: "<500ms (P95)"
    - ëª¨ë¸_ëª©ë¡: "<300ms (P95)"
  
  ì²˜ë¦¬ëŸ‰:
    - ë™ì‹œ_ì‚¬ìš©ì: "500ëª…"
    - ì´ˆë‹¹_ìš”ì²­: "1000 RPS"
    - ëŒ€ëŸ‰_ê¶Œí•œ_ì²˜ë¦¬: "100ê°œ/ì´ˆ"
  
  ê°€ìš©ì„±:
    - ì—…íƒ€ì„: "99.9%"
    - ì—ëŸ¬ìœ¨: "<0.1%"
    - ë³µêµ¬ì‹œê°„: "<5ë¶„"

ì‹œìŠ¤í…œ_ë¦¬ì†ŒìŠ¤:
  CPU: "<70% (í‰ê· )"
  ë©”ëª¨ë¦¬: "<80% (í‰ê· )"  
  ë””ìŠ¤í¬: "<80%"
  ë„¤íŠ¸ì›Œí¬: "<100Mbps"
```

#### ë¶€í•˜ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

```python
# load_test.py
import asyncio
import aiohttp
import time
from statistics import mean, stdev

async def test_permission_api(session, model_id, user_count=100):
    """ê¶Œí•œ API ë¶€í•˜ í…ŒìŠ¤íŠ¸"""
    start_time = time.time()
    tasks = []
    
    for i in range(user_count):
        task = session.get(f"/api/v2/llm-models/{model_id}/permissions",
                          headers={"Authorization": f"Bearer {get_test_token()}"})
        tasks.append(task)
    
    responses = await asyncio.gather(*tasks, return_exceptions=True)
    end_time = time.time()
    
    # ê²°ê³¼ ë¶„ì„
    response_times = []
    success_count = 0
    error_count = 0
    
    for response in responses:
        if isinstance(response, Exception):
            error_count += 1
        else:
            success_count += 1
            response_times.append(response.elapsed.total_seconds())
    
    return {
        "total_time": end_time - start_time,
        "success_rate": success_count / user_count * 100,
        "error_rate": error_count / user_count * 100,
        "avg_response_time": mean(response_times) if response_times else 0,
        "p95_response_time": sorted(response_times)[int(len(response_times) * 0.95)] if response_times else 0
    }

async def run_load_tests():
    """ì „ì²´ ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    async with aiohttp.ClientSession(base_url="http://localhost:8000") as session:
        # ê¶Œí•œ ì¡°íšŒ í…ŒìŠ¤íŠ¸
        permission_results = await test_permission_api(session, "test-model-id", 500)
        
        # ëŒ€ëŸ‰ ê¶Œí•œ ë¶€ì—¬ í…ŒìŠ¤íŠ¸
        batch_results = await test_batch_permissions(session, 100)
        
        print("ğŸ“Š ë¶€í•˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        print(f"ê¶Œí•œ ì¡°íšŒ - ì„±ê³µë¥ : {permission_results['success_rate']:.1f}%, "
              f"í‰ê·  ì‘ë‹µì‹œê°„: {permission_results['avg_response_time']*1000:.1f}ms")
        print(f"ëŒ€ëŸ‰ ì²˜ë¦¬ - ì„±ê³µë¥ : {batch_results['success_rate']:.1f}%, "
              f"P95 ì‘ë‹µì‹œê°„: {batch_results['p95_response_time']*1000:.1f}ms")

if __name__ == "__main__":
    asyncio.run(run_load_tests())
```

### 5.5 ìœ„í—˜ ê´€ë¦¬ ë° ë¡¤ë°± ê³„íš

#### ìœ„í—˜ ìš”ì†Œ ë¶„ë¥˜

| ìœ„í—˜ë„ | ìœ„í—˜ ìš”ì†Œ | ì˜í–¥ë„ | í™•ë¥  | ì™„í™” ì „ëµ |
|--------|----------|--------|------|----------|
| ë†’ìŒ | ë°ì´í„°ë² ì´ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨ | ë†’ìŒ | ì¤‘ê°„ | ë°±ì—…/ë³µêµ¬ ê³„íš, ë‹¨ê³„ì  ë§ˆì´ê·¸ë ˆì´ì…˜ |
| ë†’ìŒ | API í˜¸í™˜ì„± ë¬¸ì œ | ë†’ìŒ | ë‚®ìŒ | API ë²„ì „ ê´€ë¦¬, ì ì§„ì  ë§ˆì´ê·¸ë ˆì´ì…˜ |
| ì¤‘ê°„ | ì„±ëŠ¥ ì €í•˜ | ì¤‘ê°„ | ì¤‘ê°„ | ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§, ìë™ ìŠ¤ì¼€ì¼ë§ |
| ì¤‘ê°„ | ìºì‹œ ì¥ì•  | ì¤‘ê°„ | ë‚®ìŒ | Redis í´ëŸ¬ìŠ¤í„°ë§, ì¥ì•  ì¡°ì¹˜ |
| ë‚®ìŒ | ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì¥ì•  | ë‚®ìŒ | ë‚®ìŒ | ë‹¤ì¤‘ ëª¨ë‹ˆí„°ë§ ì²´ê³„ |

#### ë¡¤ë°± ì‹œë‚˜ë¦¬ì˜¤

```bash
#!/bin/bash
# rollback.sh

ROLLBACK_VERSION=${1:-v1.0}

echo "ğŸ”„ Rolling back to version $ROLLBACK_VERSION..."

# 1. íŠ¸ë˜í”½ ì¤‘ë‹¨
echo "ğŸ›‘ Stopping traffic..."
nginx -s stop

# 2. ì´ì „ ë²„ì „ ì»¨í…Œì´ë„ˆ ì‹œì‘
echo "ğŸ“¦ Starting previous version containers..."
docker-compose -f docker-compose.rollback.yml up -d

# 3. ë°ì´í„°ë² ì´ìŠ¤ ë¡¤ë°± (í•„ìš”ì‹œ)
if [ "$2" = "with-db" ]; then
    echo "ğŸ—„ï¸ Rolling back database..."
    psql -f rollback_migrations.sql
fi

# 4. í—¬ìŠ¤ ì²´í¬
echo "ğŸ¥ Health checking..."
timeout 30 bash -c 'until curl -f http://localhost:8000/api/health; do sleep 2; done'

# 5. íŠ¸ë˜í”½ ë³µêµ¬
echo "ğŸš¦ Restoring traffic..."
nginx -s start

echo "âœ… Rollback complete!"
```

### 5.6 íŒ€ í˜‘ì—… ë° ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ê³„íš

#### ê°œë°œíŒ€ ì—­í•  ë¶„ë‹´

```yaml
ì—­í• _ë¶„ë‹´:
  ë°±ì—”ë“œ_ê°œë°œì:
    - LLM Permissions Service ê°œë°œ
    - API V2 êµ¬í˜„
    - ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”
    - ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ êµ¬ì¶•
  
  í”„ë¡ íŠ¸ì—”ë“œ_ê°œë°œì:
    - AdminPage ë¦¬íŒ©í† ë§
    - API Client V2 ê°œë°œ
    - Token Manager êµ¬í˜„
    - UI/UX ê°œì„ 
  
  DevOps_ì—”ì§€ë‹ˆì–´:
    - ì¸í”„ë¼ êµ¬ì¶•
    - CI/CD íŒŒì´í”„ë¼ì¸
    - ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ
    - ë°°í¬ ìë™í™”
  
  QA_ì—”ì§€ë‹ˆì–´:
    - í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ì‘ì„±
    - ìë™í™” í…ŒìŠ¤íŠ¸ êµ¬ì¶•
    - ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
    - ì‚¬ìš©ì ìŠ¹ì¸ í…ŒìŠ¤íŠ¸
```

#### ì§„í–‰ ìƒí™© ì¶”ì 

```markdown
# ì£¼ê°„ ì§„í–‰ ë³´ê³ ì„œ í…œí”Œë¦¿

## Week X Progress Report

### ì™„ë£Œëœ ì‘ì—…
- [ ] Task A - ì˜ˆìƒ ì‹œê°„ vs ì‹¤ì œ ì‹œê°„
- [ ] Task B - ë°œê²¬ëœ ì´ìŠˆ ë° í•´ê²° ë°©ì•ˆ

### ì§„í–‰ ì¤‘ì¸ ì‘ì—…  
- [ ] Task C - í˜„ì¬ ì§„í–‰ë¥ , ì˜ˆìƒ ì™„ë£Œì¼
- [ ] Task D - ë¸”ë¡œì»¤ ì´ìŠˆ ë° í•´ê²° ê³„íš

### ë‹¤ìŒ ì£¼ ê³„íš
- [ ] Task E - ìš°ì„ ìˆœìœ„, ë‹´ë‹¹ì
- [ ] Task F - ì˜ì¡´ì„±, ë¦¬ìŠ¤í¬

### ì´ìŠˆ ë° ë¦¬ìŠ¤í¬
- ğŸš¨ Critical: í•´ê²° í•„ìš”
- âš ï¸ High: ëª¨ë‹ˆí„°ë§ í•„ìš”  
- â„¹ï¸ Medium: í–¥í›„ ê²€í† 

### ë©”íŠ¸ë¦­
- ì½”ë“œ ì»¤ë²„ë¦¬ì§€: X%
- í…ŒìŠ¤íŠ¸ í†µê³¼ìœ¨: X%
- ì„±ëŠ¥ ì§€í‘œ: ëª©í‘œ ëŒ€ë¹„ X%
```

### 5.7 ìµœì¢… ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

#### ê¸°ëŠ¥ ê²€ì¦

```yaml
ì¸ì¦_ë°_ê¶Œí•œ:
  - [ ] í† í° ìë™ ê°±ì‹  ì‘ë™
  - [ ] ê¶Œí•œ ë¶€ì—¬/ì·¨ì†Œ ì •ìƒ ë™ì‘
  - [ ] ì„¸ì…˜ ê´€ë¦¬ ì•ˆì •ì 

API_ì„±ëŠ¥:
  - [ ] ì‘ë‹µ ì‹œê°„ ëª©í‘œ ë‹¬ì„±
  - [ ] ì—ëŸ¬ìœ¨ ëª©í‘œ ì´í•˜ ìœ ì§€
  - [ ] ë™ì‹œ ì²˜ë¦¬ ìš©ëŸ‰ ì¶©ì¡±

ë°ì´í„°ë² ì´ìŠ¤:
  - [ ] ì¿¼ë¦¬ ì„±ëŠ¥ ìµœì í™” ì™„ë£Œ
  - [ ] ì¸ë±ìŠ¤ íš¨ê³¼ì  ì ìš©
  - [ ] ë°±ì—…/ë³µêµ¬ ê²€ì¦

ëª¨ë‹ˆí„°ë§:
  - [ ] ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‘ë™
  - [ ] ì•Œë¦¼ ì‹œìŠ¤í…œ ì •ìƒ
  - [ ] ëŒ€ì‹œë³´ë“œ ì •í™•ì„± í™•ì¸

ë³´ì•ˆ:
  - [ ] ì¸ì¦ ë³´ì•ˆ ê°•í™”
  - [ ] API ë³´ì•ˆ ê²€ì¦
  - [ ] ë°ì´í„° ì•”í˜¸í™” ì ìš©
```

#### ì‚¬ìš©ì ìŠ¹ì¸ í…ŒìŠ¤íŠ¸

```yaml
ì‹œë‚˜ë¦¬ì˜¤_í…ŒìŠ¤íŠ¸:
  ê´€ë¦¬ì_ì›Œí¬í”Œë¡œ:
    - [ ] ê·¸ë£¹ ìƒì„± ë° ìˆ˜ì •
    - [ ] LLM ëª¨ë¸ ê¶Œí•œ ë¶€ì—¬
    - [ ] ì‚¬ìš©ì ê¶Œí•œ ê´€ë¦¬
    - [ ] ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ í™•ì¸
  
  ì‚¬ìš©ì_ì›Œí¬í”Œë¡œ:
    - [ ] ë¡œê·¸ì¸/ë¡œê·¸ì•„ì›ƒ
    - [ ] ì ‘ê·¼ ê°€ëŠ¥í•œ ëª¨ë¸ í™•ì¸
    - [ ] ê¶Œí•œ ìš”ì²­ ë° ìŠ¹ì¸
  
  ì—ëŸ¬_ì‹œë‚˜ë¦¬ì˜¤:
    - [ ] ë„¤íŠ¸ì›Œí¬ ì¥ì•  ì‹œ ë³µêµ¬
    - [ ] í† í° ë§Œë£Œ ì‹œ ì²˜ë¦¬
    - [ ] ì„œë²„ ì¥ì•  ì‹œ ì•Œë¦¼
```

---

## ğŸ“‹ ì´ í”„ë¡œì íŠ¸ ìš”ì•½

### ì˜ˆìƒ ì¼ì •: 5ì£¼ (25 ì˜ì—…ì¼)
### ì˜ˆìƒ ë¦¬ì†ŒìŠ¤: 4ëª… (ë°±ì—”ë“œ 2ëª…, í”„ë¡ íŠ¸ì—”ë“œ 1ëª…, DevOps 1ëª…)
### ì˜ˆìƒ ë¹„ìš©: ê°œë°œ ì‹œê°„ + ì¸í”„ë¼ ë¹„ìš©

### ì£¼ìš” ê°œì„  ì‚¬í•­
1. **ì¸ì¦ ì‹œìŠ¤í…œ**: í† í° ìë™ ê°±ì‹ ìœ¼ë¡œ ì‚¬ìš©ì ê²½í—˜ ê°œì„ 
2. **ì„±ëŠ¥**: ì‘ë‹µ ì‹œê°„ 50% ë‹¨ì¶•, ìºì‹±ìœ¼ë¡œ DB ë¶€í•˜ ê°ì†Œ
3. **ì•ˆì •ì„±**: Circuit breakerë¡œ ì¥ì•  ê²©ë¦¬, ëª¨ë‹ˆí„°ë§ìœ¼ë¡œ ì‚¬ì „ ê°ì§€
4. **í™•ì¥ì„±**: API ë²„ì „ ê´€ë¦¬, ëŒ€ëŸ‰ ì²˜ë¦¬ ì§€ì›
5. **ìš´ì˜ì„±**: ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§, ìë™ ì•Œë¦¼, ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ

### ê¸°ëŒ€ íš¨ê³¼
- ğŸš€ ì‹œìŠ¤í…œ ì•ˆì •ì„± 95% â†’ 99.9%
- âš¡ í‰ê·  ì‘ë‹µ ì‹œê°„ 50% ê°œì„ 
- ğŸ“Š ìš´ì˜ ê°€ì‹œì„± ëŒ€í­ í–¥ìƒ
- ğŸ”§ ìœ ì§€ë³´ìˆ˜ íš¨ìœ¨ì„± ì¦ëŒ€
- ğŸ‘¥ ì‚¬ìš©ì ë§Œì¡±ë„ í–¥ìƒ

---

**ë‹¤ìŒ ë‹¨ê³„**: ê²½ì˜ì§„ ìŠ¹ì¸ í›„ Phase 1ë¶€í„° ìˆœì°¨ì  êµ¬í˜„ ì‹œì‘