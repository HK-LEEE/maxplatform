#!/usr/bin/env python3
"""
LLM ì„¤ì • í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
Azure OpenAIì™€ Ollama ì—°ê²°ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

import asyncio
import aiohttp
import os
from dotenv import load_dotenv

load_dotenv()

async def test_azure_openai():
    """Azure OpenAI ì—°ê²° í…ŒìŠ¤íŠ¸"""
    print("ğŸ”µ Azure OpenAI ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...")
    
    endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
    api_key = os.getenv("AZURE_OPENAI_API_KEY")
    deployment = os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME", "gpt-4o")
    api_version = os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-15-preview")
    
    if not endpoint or not api_key:
        print("âŒ Azure OpenAI ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤ (.env íŒŒì¼ í™•ì¸)")
        return False
    
    headers = {
        "Content-Type": "application/json",
        "api-key": api_key
    }
    
    url = f"{endpoint}/openai/deployments/{deployment}/chat/completions?api-version={api_version}"
    
    payload = {
        "messages": [{"role": "user", "content": "Hello"}],
        "max_tokens": 10
    }
    
    try:
        async with aiohttp.ClientSession() as session:
            async with session.post(url, headers=headers, json=payload, timeout=10) as response:
                if response.status == 200:
                    print("âœ… Azure OpenAI ì—°ê²° ì„±ê³µ!")
                    return True
                else:
                    error_text = await response.text()
                    print(f"âŒ Azure OpenAI ì—°ê²° ì‹¤íŒ¨ ({response.status}): {error_text}")
                    return False
    except Exception as e:
        print(f"âŒ Azure OpenAI ì—°ê²° ì˜¤ë¥˜: {e}")
        return False

async def test_ollama():
    """Ollama ì—°ê²° í…ŒìŠ¤íŠ¸"""
    print("ğŸŸ¡ Ollama ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...")
    
    base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
    default_model = os.getenv("OLLAMA_DEFAULT_MODEL", "llama3.2")
    
    try:
        # ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
        async with aiohttp.ClientSession() as session:
            async with session.get(f"{base_url}/api/tags", timeout=5) as response:
                if response.status == 200:
                    result = await response.json()
                    models = [model["name"] for model in result.get("models", [])]
                    print(f"âœ… Ollama ì—°ê²° ì„±ê³µ! ì„¤ì¹˜ëœ ëª¨ë¸: {models}")
                    
                    if default_model in [m.split(':')[0] for m in models]:
                        print(f"âœ… ê¸°ë³¸ ëª¨ë¸ '{default_model}' í™•ì¸ë¨")
                    else:
                        print(f"âš ï¸  ê¸°ë³¸ ëª¨ë¸ '{default_model}'ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                        print(f"   ì„¤ì¹˜í•˜ë ¤ë©´: ollama pull {default_model}")
                    
                    return True
                else:
                    print(f"âŒ Ollama ì„œë¹„ìŠ¤ ì˜¤ë¥˜ ({response.status})")
                    return False
    except Exception as e:
        print(f"âŒ Ollama ì—°ê²° ì˜¤ë¥˜: {e}")
        print("   Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”: ollama serve")
        return False

async def test_simple_chat():
    """ê°„ë‹¨í•œ ì±„íŒ… í…ŒìŠ¤íŠ¸"""
    print("\nğŸ¤– ê°„ë‹¨í•œ LLM ì±„íŒ… í…ŒìŠ¤íŠ¸...")
    
    provider = os.getenv("DEFAULT_LLM_PROVIDER", "ollama")
    
    if provider == "azure":
        success = await test_azure_chat()
    else:
        success = await test_ollama_chat()
    
    return success

async def test_azure_chat():
    """Azure OpenAI ì±„íŒ… í…ŒìŠ¤íŠ¸"""
    endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
    api_key = os.getenv("AZURE_OPENAI_API_KEY")
    deployment = os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME", "gpt-4o")
    api_version = os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-15-preview")
    
    headers = {
        "Content-Type": "application/json",
        "api-key": api_key
    }
    
    url = f"{endpoint}/openai/deployments/{deployment}/chat/completions?api-version={api_version}"
    
    payload = {
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Pythonì—ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì—­ìˆœìœ¼ë¡œ ë§Œë“œëŠ” ë°©ë²•ì€?"}
        ],
        "max_tokens": 100,
        "temperature": 0.1
    }
    
    try:
        async with aiohttp.ClientSession() as session:
            async with session.post(url, headers=headers, json=payload, timeout=30) as response:
                if response.status == 200:
                    result = await response.json()
                    answer = result["choices"][0]["message"]["content"]
                    print(f"âœ… Azure OpenAI ì‘ë‹µ: {answer[:100]}...")
                    return True
                else:
                    error_text = await response.text()
                    print(f"âŒ Azure OpenAI ì±„íŒ… ì‹¤íŒ¨: {error_text}")
                    return False
    except Exception as e:
        print(f"âŒ Azure OpenAI ì±„íŒ… ì˜¤ë¥˜: {e}")
        return False

async def test_ollama_chat():
    """Ollama ì±„íŒ… í…ŒìŠ¤íŠ¸"""
    base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
    model = os.getenv("OLLAMA_DEFAULT_MODEL", "llama3.2")
    
    payload = {
        "model": model,
        "prompt": "Pythonì—ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì—­ìˆœìœ¼ë¡œ ë§Œë“œëŠ” ë°©ë²•ì€? ê°„ë‹¨íˆ ë‹µí•´ì£¼ì„¸ìš”.",
        "stream": False,
        "options": {
            "temperature": 0.1,
            "num_predict": 100
        }
    }
    
    try:
        async with aiohttp.ClientSession() as session:
            async with session.post(f"{base_url}/api/generate", 
                                  json=payload, timeout=60) as response:
                if response.status == 200:
                    result = await response.json()
                    answer = result["response"]
                    print(f"âœ… Ollama ì‘ë‹µ: {answer[:100]}...")
                    return True
                else:
                    error_text = await response.text()
                    print(f"âŒ Ollama ì±„íŒ… ì‹¤íŒ¨: {error_text}")
                    return False
    except Exception as e:
        print(f"âŒ Ollama ì±„íŒ… ì˜¤ë¥˜: {e}")
        return False

async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸš€ Jupyter í”Œë«í¼ LLM ì„¤ì • í…ŒìŠ¤íŠ¸ ì‹œì‘\n")
    
    # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
    print("ğŸ“‹ í™˜ê²½ ì„¤ì • í™•ì¸:")
    print(f"   DEFAULT_LLM_PROVIDER: {os.getenv('DEFAULT_LLM_PROVIDER', 'ollama')}")
    print(f"   AZURE_OPENAI_ENDPOINT: {'ì„¤ì •ë¨' if os.getenv('AZURE_OPENAI_ENDPOINT') else 'ì—†ìŒ'}")
    print(f"   OLLAMA_BASE_URL: {os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')}")
    print()
    
    # ì—°ê²° í…ŒìŠ¤íŠ¸
    azure_ok = await test_azure_openai()
    ollama_ok = await test_ollama()
    
    # ì±„íŒ… í…ŒìŠ¤íŠ¸
    if azure_ok or ollama_ok:
        chat_ok = await test_simple_chat()
    else:
        chat_ok = False
    
    # ê²°ê³¼ ìš”ì•½
    print("\nğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½:")
    print(f"   Azure OpenAI: {'âœ… ì •ìƒ' if azure_ok else 'âŒ ì‹¤íŒ¨'}")
    print(f"   Ollama: {'âœ… ì •ìƒ' if ollama_ok else 'âŒ ì‹¤íŒ¨'}")
    print(f"   ì±„íŒ… í…ŒìŠ¤íŠ¸: {'âœ… ì •ìƒ' if chat_ok else 'âŒ ì‹¤íŒ¨'}")
    
    if not (azure_ok or ollama_ok):
        print("\nâš ï¸  ìµœì†Œ í•˜ë‚˜ì˜ LLM ì œê³µìê°€ ì„¤ì •ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")
        print("   Azure OpenAI ë˜ëŠ” Ollama ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    elif chat_ok:
        print("\nğŸ‰ LLM ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! Jupyter í”Œë«í¼ì—ì„œ AI ë„ìš°ë¯¸ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(main()) 